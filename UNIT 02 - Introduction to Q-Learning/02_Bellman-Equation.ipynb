{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman Equation: simplify our value estimation\n",
    "\n",
    "Bellman denklemi **durum değeri veya durum-eylem değeri hesaplamamızı basitleştirir.**\n",
    "\n",
    "![Bellman equation](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg)\n",
    "\n",
    "Şimdiye kadar öğrendiklerimizle, $V(S_t)$ (value of a state) hesaplarsak, o durumdan başlayarak getiriyi hesaplamamız ve ardından sonsuza kadar politikayı izlememiz gerektiğini biliyoruz. **(Aşağıdaki örnekte tanımladığımız politika bir Açgözlü Politikadır; basitleştirme için ödülü iskonto etmiyoruz).**\n",
    "\n",
    "Dolayısıyla $V(S_t)$ değerini hesaplamak için beklenen ödüllerin toplamını hesaplamamız gerekir. Dolayısıyla:\n",
    "\n",
    "![Bellman equation](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg)\n",
    "*Durum 1'in değerini hesaplamak için: ajan bu durumda başlamışsa ve daha sonra tüm zaman adımları için açgözlü politikayı (en iyi durum değerlerine yol açan eylemleri alarak) izlemişse ödüllerin toplamı.*\n",
    "\n",
    "Ardından, $V(S_{t+1})$ değerini hesaplamak için, bu durumdan başlayarak $S_{t+1}$ getirisini hesaplamamız gerekir.\n",
    "\n",
    "![Bellman equation](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg)\n",
    "*Durum 2'nin değerini hesaplamak için: **Eğer ajan bu durumda** başladıysa ve ardından tüm zaman adımları boyunca **politikayı izlediyse** ödüllerin toplamı.*\n",
    "\n",
    "Fark etmişsinizdir, farklı durumların değerini hesaplamayı tekrarlıyoruz, bu da her durum değeri veya durum-eylem değeri için yapmanız gerekiyorsa sıkıcı olabilir.\n",
    "\n",
    "Her bir durum veya her bir durum-eylem çifti için beklenen getiriyi hesaplamak yerine **Bellman denklemini kullanabiliriz.** (ipucu: Dinamik Programlamanın ne olduğunu biliyorsanız, bu çok benzer! ne olduğunu bilmiyorsanız, endişelenmeyin!)\n",
    "\n",
    "Bellman denklemi şu şekilde çalışan özyinelemeli bir denklemdir: her durum için baştan başlayıp getiriyi hesaplamak yerine, herhangi bir durumun değerini şu şekilde düşünebiliriz:\n",
    "\n",
    "**Anlık ödül $R_{t+1}$ + takip eden durumun indirgenmiş değeri ( $\\gamma * V(S_{t+1}) $ ) .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
