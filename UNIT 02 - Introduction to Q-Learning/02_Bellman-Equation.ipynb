{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman Equation: simplify our value estimation\n",
    "\n",
    "Bellman denklemi **durum değeri veya durum-eylem değeri hesaplamamızı basitleştirir.**\n",
    "\n",
    "![Bellman equation](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg)\n",
    "\n",
    "Şimdiye kadar öğrendiklerimizle, $V(S_t)$ (value of a state) hesaplarsak, o durumdan başlayarak getiriyi hesaplamamız ve ardından sonsuza kadar politikayı izlememiz gerektiğini biliyoruz. **(Aşağıdaki örnekte tanımladığımız politika bir Açgözlü Politikadır; basitleştirme için ödülü discount etmiyoruz).**\n",
    "\n",
    "Dolayısıyla $V(S_t)$ değerini hesaplamak için beklenen ödüllerin toplamını hesaplamamız gerekir. Dolayısıyla:\n",
    "\n",
    "![Bellman equation](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg)\n",
    "*Durum 1'in değerini hesaplamak için: ajan bu durumda başlamışsa ve daha sonra tüm zaman adımları için açgözlü politikayı (en iyi durum değerlerine yol açan eylemleri alarak) izlemişse ödüllerin toplamı.*\n",
    "\n",
    "Ardından, $V(S_{t+1})$ değerini hesaplamak için, bu durumdan başlayarak $S_{t+1}$ getirisini hesaplamamız gerekir.\n",
    "\n",
    "![Bellman equation](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg)\n",
    "*Durum 2'nin değerini hesaplamak için: **Eğer ajan bu durumda** başladıysa ve ardından tüm zaman adımları boyunca **politikayı izlediyse** ödüllerin toplamı.*\n",
    "\n",
    "Fark etmişsinizdir, farklı durumların değerini hesaplamayı tekrarlıyoruz, bu da her durum değeri veya durum-eylem değeri için yapmanız gerekiyorsa sıkıcı olabilir.\n",
    "\n",
    "Her bir durum veya her bir durum-eylem çifti için beklenen getiriyi hesaplamak yerine **Bellman denklemini kullanabiliriz.** (ipucu: Dinamik Programlamanın ne olduğunu biliyorsanız, bu çok benzer! ne olduğunu bilmiyorsanız, endişelenmeyin!)\n",
    "\n",
    "Bellman denklemi şu şekilde çalışan özyinelemeli bir denklemdir: her durum için baştan başlayıp getiriyi hesaplamak yerine, herhangi bir durumun değerini şu şekilde düşünebiliriz:\n",
    "\n",
    "**Anlık ödül $R_{t+1}$ + takip eden durumun indirgenmiş değeri ( $\\gamma * V(S_{t+1}) $ ) .**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg)\n",
    "\n",
    "Örneğimize geri dönecek olursak, Durum 1'in değerinin, o durumdan başlamamız halinde beklenen kümülatif getiriye eşit olduğunu söyleyebiliriz.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg)\n",
    "\n",
    "Durum 1'in değerini hesaplamak için: ajan bu durum 1'de başladıysa ve daha sonra tüm zaman adımları boyunca politikayı izlediyse ödüllerin toplamı.\n",
    "\n",
    "$V(S_T)$ = Immediate reward $R_{t+1}$ + Discounted value of the next state $\\gamma * V(S_{t+1})$\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman6.jpg)\n",
    "*For simplification, here we don’t discount so gamma = 1.*\n",
    "\n",
    "Basitlik adına, burada discount yapmıyoruz, yani gamma = 1. Ancak bu ünitenin Q-Learning bölümünde gama = 0,99 olan bir örnek üzerinde çalışacaksınız.\n",
    "\n",
    "- The value of  $V(S_{t+1}) $  = Immediate reward  $R_{t+2}$  + Discounted value of the next state ( $gamma * V(S_{t+2})$ ).\n",
    "- And so on.\n",
    "\n",
    "Özetlemek gerekirse, Bellman denkleminin ana fikri, her bir değeri beklenen getirinin toplamı olarak hesaplamak yerine, ki bu uzun bir süreçtir, değeri anlık ödül + takip eden durumun iskonto edilmiş değerinin toplamı olarak hesaplamaktır.\n",
    "\n",
    "Bir sonraki bölüme geçmeden önce, Bellman denkleminde gammanın rolü hakkında düşünün. Gama değeri çok düşükse (örneğin 0,1 veya hatta 0) ne olur? Değer 1 ise ne olur? Değer çok yüksekse, örneğin bir milyon ise ne olur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
