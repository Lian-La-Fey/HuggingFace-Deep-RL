{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Q-Learning example\n",
    "\n",
    "Q-Learning'i daha iyi anlamak için basit bir örnek verelim:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-Example-2.jpg)\n",
    "\n",
    "- Bu küçük labirentte bir faresin. Her zaman **aynı başlangıç noktasından başlıyorsun**\n",
    "- Amacınız **sağ alt köşedeki büyük peynir yığınını yemek** ve zehirden kaçınmak. \n",
    "- Zehri yersek, **büyük peynir yığınını** yersek veya beş adımdan fazla atarsak bölüm sona erer.\n",
    "- Öğrenme oranı 0,1\n",
    "- İskonto oranı (gamma) 0,99\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-1.jpg)\n",
    "\n",
    "The reward function goes like this:\n",
    "\n",
    "- **+0:** İçinde hiç peynir olmayan bir duruma gitmek.\n",
    "- **+1:** İçinde küçük bir peynir olan bir duruma gitmek.\n",
    "- **+10:** Büyük peynir yığınının olduğu duruma gitmek.\n",
    "- **-10:** Zehirli duruma gitmek ve böylece ölmek.\n",
    "- **+0** Eğer beş adımdan fazla atarsak.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-2.jpg)\n",
    "\n",
    "Temsilcimizi optimal bir politikaya sahip olacak şekilde eğitmek için (yani sağa, sağa, aşağı giden bir politika), **Q-Learning algoritmasını** kullanacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 1: Initialize the Q-table\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-1.jpg)\n",
    "\n",
    "Dolayısıyla, şimdilik **Q-tablomuz işe yaramaz**; Q-Öğrenme algoritmasını kullanarak Q-fonksiyonumuzu eğitmemiz gereki.\n",
    "\n",
    "Bunu 2 eğitim zaman adımı için yapalım:\n",
    "\n",
    "Eğitim zaman adımı 1:\n",
    "\n",
    "## Step 2: Choose an action using the Epsilon Greedy Strategy\n",
    "\n",
    "Epsilon büyük olduğu için (= 1.0), rastgele bir eylem yapıyorum. Bu durumda, sağa giderim.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-3.jpg)\n",
    "\n",
    "## Step 3: Perform action At, get Rt+1 and St+1\n",
    "\n",
    "Sağa giderek küçük bir peynir elde ediyorum, yani $R_{t+1} = 1$ ve yeni bir durumdayım.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-4.jpg)\n",
    "\n",
    "## Step 4: Update Q(St, At)\n",
    "\n",
    "Şimdi formülümüzü kullanarak $Q(S_t, A_t)$ değerini güncelleyebiliriz.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg)\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-4.jpg)\n",
    "\n",
    "Eğitim zaman adımı 2:\n",
    "\n",
    "## Step 2: Choose an action using the Epsilon Greedy Strategy\n",
    "\n",
    "**epsilon=0.99 büyük olduğu için tekrar rastgele bir eylem gerçekleştiriyorum**. (Epsilonu biraz azalttığımıza dikkat edin çünkü eğitim ilerledikçe daha az keşif istiyoruz).\n",
    "\n",
    "'Aşağı' eylemini gerçekleştirdim. **Bu iyi bir eylem değil çünkü beni zehre götürüyor.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-6.jpg)\n",
    "\n",
    "## Step 3: Perform action At, get Rt+1 and St+1\n",
    "\n",
    "Zehir yediğim için, **R_{t+1} = -10$ alırım ve ölürüm.**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-7.jpg)\n",
    "\n",
    "## Step 4: Update Q(St, At)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-8.jpg)\n",
    "\n",
    "Çünkü öldük, yeni bir bölüme başlıyoruz. Ancak burada gördüğümüz şey, **iki keşif adımıyla ajanımın daha akıllı hale geldiğidir.**\n",
    "\n",
    "Çevreyi keşfetmeye ve kullanmaya devam ettikçe ve TD hedefini kullanarak Q değerlerini güncelledikçe, **Q tablosu bize daha iyi ve daha iyi bir yaklaşım verecektir. Eğitimin sonunda, optimum Q-fonksiyonunun bir tahminini elde edeceğiz.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
