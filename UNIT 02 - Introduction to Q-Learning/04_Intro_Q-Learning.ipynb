{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Q-Learning\n",
    "\n",
    "## What is Q-Learning?\n",
    "\n",
    "Q-Learning, **eylem-değer fonksiyonunu eğitmek için bir TD yaklaşımı kullanan politika dışı değer tabanlı bir yöntemdir:**\n",
    "\n",
    "- *Politika dışı*: bu ünitenin sonunda bundan bahsedeceğiz \n",
    "- *Değer tabanlı yöntem*: bize **her bir durumun veya her bir durum-eylem çiftinin değerini söyleyecek bir değer veya eylem-değer fonksiyonunu eğiterek dolaylı olarak en uygun politikayı bulur.**\n",
    "- *TD yaklaşımı:* **eylem-değer fonksiyonunu bölümün sonunda değil her adımda günceller.**\n",
    "\n",
    "**Q-Learning, belirli bir durumda olmanın ve o durumda belirli bir eylemde bulunmanın değerini belirleyen bir **eylem-değer fonksiyonu** olan Q-fonksiyonumuzu** eğitmek için kullandığımız algoritmadır.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg)\n",
    "\n",
    "Q, o eylemin o durumdaki \"Quality\" (değerinden) gelir.**\n",
    "\n",
    "Değer ve ödül arasındaki farkı özetleyelim:\n",
    "\n",
    "- Bir durumun ya da bir *durum-eylem çiftinin* değeri, ajanımızın bu durumdan (ya da durum-eylem çiftinden) başlayıp politikasına göre hareket etmesi halinde elde edeceği beklenen kümülatif ödüldür - *ödül*, bir durumda bir eylem gerçekleştirdikten sonra **çevreden aldığım** geribildirimdir.\n",
    "\n",
    "Dahili olarak, Q-fonksiyonumuz **her hücrenin bir durum-eylem çifti değerine karşılık geldiği bir tablo olan bir Q-tablosu ile kodlanır.** Bu Q-tablosunu **Q-fonksiyonumuzun hafızası veya kopya kağıdı olarak düşünün.**\n",
    "\n",
    "Bir labirent örneği üzerinden gidelim.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg)\n",
    "\n",
    "Q-tablosu başlatılmıştır. Bu nedenle tüm değerler = 0'dır. Bu tablo, her durum ve eylem için karşılık gelen durum-eylem değerlerini içerir. Bu basit örnek için, durum yalnızca farenin konumu ile tanımlanır. Bu nedenle, Q-tablomuzda 2*3 satırımız var, farenin her olası konumu için bir satır. Daha karmaşık senaryolarda, durum aktörün konumundan daha fazla bilgi içerebilir.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg)\n",
    "\n",
    "Burada başlangıç durumunun ve yukarı çıkmanın durum-eylem değerinin 0 olduğunu görüyoruz:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg)\n",
    "\n",
    "Yani: Q-fonksiyonu her durum-eylem çiftinin değerini içeren bir Q-tablosu kullanır. Bir durum ve eylem verildiğinde, Q-fonksiyonumuz değeri çıktılamak için Q-tablosunun içinde arama yapacaktır.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg)\n",
    "\n",
    "Özetleyecek olursak, *Q-Öğrenme* **aşağıdaki RL algoritmasıdır:**\n",
    "\n",
    "- Dahili olarak tüm durum-eylem çifti değerlerini içeren bir **Q-tablosu** olan bir *Q-fonksiyonu* (bir **eylem-değer fonksiyonu**) eğitir.**\n",
    "- Bir durum ve eylem verildiğinde, Q-fonksiyonumuz **karşılık gelen değeri bulmak için Q-tablosunda arama yapar.** \n",
    "- Eğitim tamamlandığında, **optimal bir Q-fonksiyonuna sahip oluruz, bu da optimal Q-tablosuna sahip olduğumuz anlamına gelir.** \n",
    "- Ve eğer **optimal bir Q-fonksiyonuna** sahipsek, **optimal bir politikaya** sahip oluruz, çünkü **her durumda yapılacak en iyi eylemi biliriz.**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg)\n",
    "\n",
    "Başlangıçta, Q-tablomuz her durum-eylem çifti için keyfi değerler verdiğinden işe yaramaz (çoğu zaman Q-tablosunu 0 olarak başlatırız). Temsilci çevreyi keşfettikçe ve biz Q-tablosunu güncelledikçe, bu bize optimal politikaya giderek daha iyi bir yaklaşım sağlayacaktır.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg)\n",
    "\n",
    "Artık Q-Learning, Q-fonksiyonları ve Q-tablolarının ne olduğunu anladığımıza göre, Q-Learning algoritmasının derinliklerine inelim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The Q-Learning algorithm\n",
    "\n",
    "Bu Q-Learning sözde kodudur; uygulamadan önce her bir parçayı inceleyelim ve basit bir örnekle nasıl çalıştığını görelim. Bu gözünüzü korkutmasın, göründüğünden daha basittir! Her adımın üzerinden geçeceğiz.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg)\n",
    "\n",
    "**Step 1: We initialize the Q-table**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg)\n",
    "Her durum-eylem çifti için Q-tablosunu başlatmamız gerekir. Çoğu zaman 0 değeriyle başlatırız.\n",
    "\n",
    "**Step 2: Choose an action using the epsilon-greedy strategy**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg)\n",
    "\n",
    "Epsilon-greedy stratejisi, keşif/sömürü dengesini ele alan bir politikadır.\n",
    "\n",
    "Buradaki fikir, ɛ = 1.0 başlangıç değeri ile:\n",
    "\n",
    "- 1 - ɛ* olasılığı ile: **sömürü** yaparız (diğer bir deyişle ajanımız en yüksek durum-eylem çifti değerine sahip eylemi seçer) \n",
    "- ɛ olasılığı ile: **keşif** yaparız (rastgele eylem deneriz).\n",
    "\n",
    "Eğitimin başında, ɛ çok yüksek olduğu için **keşif yapma olasılığı çok büyük olacaktır, bu nedenle çoğu zaman keşif yapacağız.** Ancak eğitim devam ettikçe ve sonuç olarak **Q-tablomuz tahminlerinde gittikçe daha iyi hale geldikçe, epsilon değerini** giderek azaltıyoruz çünkü daha az keşfe ve daha fazla sömürüye ihtiyacımız olacak.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg)\n",
    "\n",
    "**Step 3: Perform action At, get reward Rt+1 and next state St+1**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg)\n",
    "\n",
    "**Step 4: Update Q(St, At)**\n",
    "\n",
    "TD Öğrenmede, politikamızı veya değer fonksiyonumuzu (seçtiğimiz RL yöntemine bağlı olarak) **etkileşimin bir adımından sonra** güncellediğimizi unutmayın.\n",
    "\n",
    "TD hedefimizi üretmek için, **anlık ödül $R_{t+1}$ artı bir sonraki durumun iskonto edilmiş değerini** kullandık, bir sonraki durumda mevcut Q-fonksiyonunu maksimize eden eylemi bularak hesapladık. (Biz buna bootstrap diyoruz).\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg)\n",
    "\n",
    "Bu nedenle, $Q(S_t, A_t)$ **güncelleme formülü şu şekildedir:**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg)\n",
    "\n",
    "Bu, $Q(S_t, A_t)$ değerimizi güncellemek anlamına gelir:\n",
    "\n",
    "- $S_t, A_t, R_{t+1}, S_{t+1}$'e ihtiyacımız var. \n",
    "- Belirli bir durum-eylem çiftindeki Q değerimizi güncellemek için TD hedefini kullanırız.\n",
    "\n",
    "TD hedefini nasıl oluştururuz? \n",
    "1. $A_t$ eylemini gerçekleştirdikten sonra $R_{t+1}$ ödülünü elde ederiz. \n",
    "2. Bir sonraki durum için **en iyi durum-eylem çifti değerini** elde etmek amacıyla, bir sonraki en iyi eylemi seçmek için açgözlü bir politika kullanırız. Bunun bir epsilon-greedy politikası olmadığına dikkat edin, bu her zaman en yüksek durum-eylem değerine sahip eylemi alacaktır.\n",
    "\n",
    "Daha sonra bu Q-değerinin güncellemesi tamamlandığında, yeni bir durumda başlarız ve eylemimizi **yine bir epsilon-greedy politikası kullanarak** seçeriz.\n",
    "\n",
    "**Bu yüzden Q Learning'in politika dışı bir algoritma olduğunu söylüyoruz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Off-policy vs On-policy [[off-vs-on]]\n",
    "\n",
    "Aradaki fark çok ince:\n",
    "\n",
    "- *Off-policy*: **eylem (çıkarım) ve güncelleme (eğitim) için farklı bir politika** kullanmak.\n",
    "\n",
    "Örneğin, Q-Learning ile epsilon-greedy politikası ( eylem politikası), Q-değerimizi güncellemek için en iyi sonraki durum eylem değerini seçmek için kullanılan greedy politikasından (güncelleme politikası) farklıdır.**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg)\n",
    "\n",
    "Eğitim bölümünde kullandığımız politikadan farklıdır:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg)\n",
    "\n",
    "- *On-policy:* *eylem ve güncelleme için aynı politika** kullanmak.\n",
    "\n",
    "Örneğin, başka bir değer tabanlı algoritma olan Sarsa'da **epsilon-greedy politikası bir sonraki durum-eylem çiftini seçer, greedy bir politika değil, **.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg)\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
