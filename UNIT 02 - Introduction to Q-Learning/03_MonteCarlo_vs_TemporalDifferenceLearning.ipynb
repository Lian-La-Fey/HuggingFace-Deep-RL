{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo vs Temporal Difference Learning\n",
    "\n",
    "Q-Learning'e geçmeden önce tartışmamız gereken son şey iki öğrenme stratejisidir.\n",
    "\n",
    "Bir RL ajanının **çevresiyle etkileşime girerek öğrendiğini unutmayın.** Buradaki fikir, **deneyim ve alınan ödül göz önüne alındığında, ajanın değer fonksiyonunu veya politikasını güncelleyeceğidir.**\n",
    "\n",
    "Monte Carlo ve Temporal Difference Learning, değer fonksiyonumuzu veya politika fonksiyonumuzu nasıl eğiteceğimize dair iki farklı **stratejidir.** Her ikisi de **RL problemini çözmek için deneyimi kullanır.**\n",
    "\n",
    "Bir yandan, Monte Carlo öğrenmeden önce **tüm bir deneyim bölümünü kullanır.** Diğer yandan, Temporal Difference öğrenmek için **sadece bir adım ( $S_t, A_t, R_{t+1}, S_{t+1}$ ) kullanır.**\n",
    "\n",
    "Her ikisini de **değer tabanlı bir yöntem örneği kullanarak açıklayacağız.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Monte Carlo: learning at the end of the episode\n",
    "\n",
    "Monte Carlo **bölümün sonuna kadar bekler**, $G_t$ (return) hesaplar ve bunu $V(S_t)$ güncellemesi için bir hedef olarak kullanır.**\n",
    "\n",
    "Dolayısıyla, değer fonksiyonumuzu güncellemeden önce **tam bir etkileşim bölümü gerektirir.**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg)\n",
    "\n",
    "Bir örnek verecek olursak:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-2.jpg)\n",
    "\n",
    "- Bölümü her zaman **aynı başlangıç noktasında başlatırız.** \n",
    "- **Ajan politikayı kullanarak eylemler gerçekleştirir**. Örneğin, keşif (rastgele eylemler) ve sömürü arasında değişen bir politika olan EEpsilon Greedy Strategy kullanarak \n",
    "- **Ödülü ve bir sonraki durumu** alırız.\n",
    "- Kedi fareyi yerse veya fare > 10 adım hareket ederse bölümü sonlandırırız.\n",
    "\n",
    "- Bölümün sonunda, **Durum, Eylemler, Ödüller ve Sonraki Durumlar tuple'larının bir listesine sahibiz** Örneğin [[Durum karosu 3 alt, Sola Git, +1, Durum karosu 2 alt], [Durum karosu 2 alt, Sola Git, +0, Durum karosu 1 alt]...]\n",
    "\n",
    "- Temsilci toplam ödülleri $G_t$** toplayacaktır (ne kadar iyi yaptığını görmek için) \n",
    "- Daha sonra **formüle göre $V(s_t)$'yi güncelleyecektir**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3.jpg)\n",
    "\n",
    "- Sonra bu yeni bilgiyle yeni bir oyuna başlayın\n",
    "\n",
    "Daha fazla bölüm yürüterek, ajan daha iyi ve daha iyi oynamayı öğrenecektir.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg)\n",
    "\n",
    "Örneğin, Monte Carlo kullanarak bir durum-değer fonksiyonunu eğitirsek:\n",
    "\n",
    "- Değer fonksiyonumuzu **her durum için 0 değeri döndürecek şekilde** başlatırız \n",
    "- Öğrenme oranımız (lr) 0,1 ve iskonto oranımız 1'dir (= iskonto yok) \n",
    "- Faremiz **çevreyi keşfeder ve rastgele eylemler gerçekleştirir**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4.jpg)\n",
    "\n",
    "- Fare 10'dan fazla adım attı, böylece bölüm sona erdi.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4p.jpg)\n",
    "\n",
    "- Elimizde durum, eylem, ödüller, sonraki_durum listesi var, **geri dönüşü hesaplamamız gerekiyor $G{t=0}$**\n",
    "\n",
    "$G_t = R_{t+1} + R_{t+2} + R_{t+3} ...$ (basitlik için ödülleri iskonto etmiyoruz)\n",
    "\n",
    "$G_0 = R_{1} + R_{2} + R_{3}...$\n",
    "\n",
    "$G_0 = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0$\n",
    "\n",
    "$G_0 = 3$\n",
    "\n",
    "- Şimdi **yeni** $V(S_0)$ değerini hesaplayabiliriz:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5.jpg)\n",
    "\n",
    "$V(S_0) = V(S_0) + lr * [G_0 — V(S_0)]$\n",
    "\n",
    "$V(S_0) = 0 + 0.1 * [3 – 0]$\n",
    "\n",
    "$V(S_0) = 0.3$\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5p.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Temporal Difference Learning: learning at each step\n",
    "\n",
    "Öte yandan **Temporal Difference, TD hedefi oluşturmak ve $R_{t+1}$ ve $ \\gamma * V(S_{t+1})$ kullanarak $V(S_t)$'yi güncellemek için yalnızca bir etkileşim (bir adım) $S_{t+1}$** bekler.\n",
    "\n",
    "TD ile fikir, **$V(S_t)$'yi her adımda güncellemektir**.\n",
    "\n",
    "Ancak tüm bir dönemi deneyimlemediğimiz için $G_t$ (beklenen getiri) değerine sahip değiliz. Bunun yerine, **$R_{t+1}$ ve bir sonraki durumun iskonto edilmiş değerini ekleyerek $G_t$ değerini tahmin ederiz.**\n",
    "\n",
    "Buna bootstrapping denir. TD, güncellemesini tam bir $G_t$ örneğine değil, kısmen mevcut bir $V(S_{t+1})$ tahminine dayandırdığı için **bu şekilde adlandırılır.**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg)\n",
    "\n",
    "Bu yönteme TD(0) veya tek adımlı TD (her bir adımdan sonra değer fonksiyonunun güncellenmesi) adı verilir.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg)\n",
    "\n",
    "Aynı örneği ele alırsak,\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2.jpg)\n",
    "\n",
    "- Değer fonksiyonumuzu her durum için 0 değerini döndürecek şekilde başlatıyoruz \n",
    "- Öğrenme oranımız (lr) 0,1 ve iskonto oranımız 1 (iskonto yok) \n",
    "- Faremiz çevreyi keşfetmeye başlar ve rastgele bir eylemde bulunur: **Sola doğru gidiyor** \n",
    "- Bir parça peynir yediği için $R_{t+1} = 1$ ödülünü alıyor**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2p.jpg)\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg)\n",
    "\n",
    "Şimdi $V(S_0)$'ı güncelleyebiliriz:\n",
    "\n",
    "Yeni $V(S_0) = V(S_0) + lr * [R_1 + \\gamma * V(S_1) - V(S_0)]$\n",
    "\n",
    "Yeni $V(S_0) = 0 + 0,1 * [1 + 1 * 0-0]$\n",
    "\n",
    "Yeni $V(S_0) = 0,1$\n",
    "\n",
    "Böylece 0. Durum için değer fonksiyonumuzu güncelledik.\n",
    "\n",
    "Şimdi **güncellenmiş değer fonksiyonumuzla bu ortamla etkileşime devam ediyoruz.**\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3p.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Özetlemek gerekirse:\n",
    "\n",
    "  - Monte Carlo* ile, değer fonksiyonunu tam bir bölümden güncelleriz ve böylece **bu bölümün gerçek doğru indirgenmiş getirisini kullanırız.** \n",
    "  \n",
    "  - *TD Öğrenme* ile, değer fonksiyonunu bir adımdan güncelleriz ve bilmediğimiz $(G_t)$ yerine **TD hedefi adı verilen tahmini bir getiri koyarız.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
