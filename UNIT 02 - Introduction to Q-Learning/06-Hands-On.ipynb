{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hands-on\n\nQ-Learning algoritmasÄ±nÄ± incelediÄŸimize gÃ¶re, ÅŸimdi onu sÄ±fÄ±rdan uygulayalÄ±m ve Q-Learning ajanÄ±mÄ±zÄ± iki ortamda eÄŸitelim:\n\n1. Frozen-Lake-v1 (kaygan olmayan ve kaygan versiyon) â˜ƒï¸ : burada ajanÄ±mÄ±zÄ±n baÅŸlangÄ±Ã§ durumundan (S) hedef durumuna (G) sadece donmuÅŸ karolar (F) Ã¼zerinde yÃ¼rÃ¼yerek ve deliklerden (H) kaÃ§Ä±narak gitmesi gerekecektir.\n\n2. Otonom bir taksinin ğŸš– yolcularÄ±nÄ± A noktasÄ±ndan B noktasÄ±na taÅŸÄ±mak iÃ§in bir ÅŸehirde gezinmeyi Ã¶ÄŸrenmesi gerekecektir.\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš•\n\n### ğŸ® Environments:\n\n- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n\n### ğŸ“š RL-Library:\n\n- Python and NumPy\n- [Gymnasium](https://gymnasium.farama.org/)\n\n## Objectives of this notebook ğŸ†\n\nNot defterinin sonunda, ÅŸunlarÄ± yapacaksÄ±nÄ±z:\n\n- Ortam kÃ¼tÃ¼phanesi olan **Gymnasium**'u kullanabilecek \n- SÄ±fÄ±rdan bir Q-Learning ajanÄ±nÄ± kodlayabilecek \n- EÄŸitilmiÅŸ ajanÄ±nÄ±zÄ± ve kodunuzu gÃ¼zel bir video tekrarÄ± ve bir deÄŸerlendirme puanÄ± ile **Hub'a** gÃ¶nderebileceksiniz ğŸ”¥.","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:30:48.617643Z","iopub.execute_input":"2024-08-13T11:30:48.618080Z","iopub.status.idle":"2024-08-13T11:31:15.862969Z","shell.execute_reply.started":"2024-08-13T11:30:48.618045Z","shell.execute_reply":"2024-08-13T11:31:15.861622Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (0.29.0)\nCollecting pygame (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 2))\n  Downloading pygame-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 3)) (1.26.4)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (0.23.4)\nCollecting pickle5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 6))\n  Downloading pickle5-0.0.11.tar.gz (132 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pyyaml==6.0 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 7))\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (2.33.1)\nCollecting imageio_ffmpeg (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9))\n  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\nCollecting pyglet==1.5.1 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 10))\n  Downloading pyglet-1.5.1-py2.py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 11)) (4.66.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (2.2.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (4.9.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (0.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2.32.3)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (9.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio_ffmpeg->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2024.7.4)\nDownloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyglet-1.5.1-py2.py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pygame-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pickle5\n  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=125214 sha256=52c9d7bb57d1d17187d889abd319ce2b5c147d071433edeff23922fcc5f27b73\n  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\nSuccessfully built pickle5\nInstalling collected packages: pyglet, pickle5, pyyaml, pygame, imageio_ffmpeg\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.1\n    Uninstalling PyYAML-6.0.1:\n      Successfully uninstalled PyYAML-6.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imageio_ffmpeg-0.5.1 pickle5-0.0.11 pygame-2.6.0 pyglet-1.5.1 pyyaml-6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg xvfb\n!pip3 install -q pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:31:15.864885Z","iopub.execute_input":"2024-08-13T11:31:15.865200Z","iopub.status.idle":"2024-08-13T11:31:52.300487Z","shell.execute_reply.started":"2024-08-13T11:31:15.865167Z","shell.execute_reply":"2024-08-13T11:31:52.298950Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Get:1 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]\nGet:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [1616 B]       \nGet:3 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]      \nHit:4 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:5 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [25.6 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:7 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3160 kB]\nGet:8 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1496 kB]\nGet:9 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1256 kB]\nHit:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4018 kB]\nGet:12 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3867 kB]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3885 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4351 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1543 kB]\nFetched 23.9 MB in 4s (6129 kB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  freeglut3 libglu1-mesa\nSuggested packages:\n  python3-numpy libgle3\nThe following NEW packages will be installed:\n  freeglut3 libglu1-mesa python3-opengl\n0 upgraded, 3 newly installed, 0 to remove and 86 not upgraded.\nNeed to get 728 kB of archives.\nAfter this operation, 6216 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libglu1-mesa amd64 9.0.1-1build1 [168 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python3-opengl all 3.1.0+dfsg-2build1 [486 kB]\nFetched 728 kB in 2s (379 kB/s)        \nSelecting previously unselected package freeglut3:amd64.\n(Reading database ... 110195 files and directories currently installed.)\nPreparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\nUnpacking freeglut3:amd64 (2.8.1-3) ...\nSelecting previously unselected package libglu1-mesa:amd64.\nPreparing to unpack .../libglu1-mesa_9.0.1-1build1_amd64.deb ...\nUnpacking libglu1-mesa:amd64 (9.0.1-1build1) ...\nSelecting previously unselected package python3-opengl.\nPreparing to unpack .../python3-opengl_3.1.0+dfsg-2build1_all.deb ...\nUnpacking python3-opengl (3.1.0+dfsg-2build1) ...\nSetting up freeglut3:amd64 (2.8.1-3) ...\nSetting up libglu1-mesa:amd64 (9.0.1-1build1) ...\nSetting up python3-opengl (3.1.0+dfsg-2build1) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.14) ...\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\nxvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.17).\n0 upgraded, 0 newly installed, 0 to remove and 86 not upgraded.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Yeni yÃ¼klenen kÃ¼tÃ¼phanelerin kullanÄ±ldÄ±ÄŸÄ±ndan emin olmak iÃ§in bazen notebook Ã§alÄ±ÅŸma zamanÄ±nÄ± yeniden baÅŸlatmak gerekir. Bir sonraki hÃ¼cre Ã§alÄ±ÅŸma zamanÄ±nÄ± Ã§Ã¶kmeye zorlayacaktÄ±r, bu nedenle tekrar baÄŸlanmanÄ±z ve kodu buradan baÅŸlayarak Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekecektir. Bu hile sayesinde sanal ekranÄ±mÄ±zÄ± Ã§alÄ±ÅŸtÄ±rabileceÄŸiz.","metadata":{}},{"cell_type":"code","source":"import os\n\nos.kill(os.getpid(), 9)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:20:48.951063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:31:52.302939Z","iopub.execute_input":"2024-08-13T11:31:52.303419Z","iopub.status.idle":"2024-08-13T11:31:52.715523Z","shell.execute_reply.started":"2024-08-13T11:31:52.303379Z","shell.execute_reply":"2024-08-13T11:31:52.714384Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<pyvirtualdisplay.display.Display at 0x78996cdbdb40>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Importing Necessary Packages","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport gymnasium as gym\nimport random\nimport imageio\nimport tqdm\nimport pickle5 as pickle\n\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:32.554187Z","iopub.execute_input":"2024-08-13T11:32:32.554607Z","iopub.status.idle":"2024-08-13T11:32:33.308246Z","shell.execute_reply.started":"2024-08-13T11:32:32.554574Z","shell.execute_reply":"2024-08-13T11:32:33.306778Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: Frozen Lake â›„ (non slippery version)","metadata":{}},{"cell_type":"markdown","source":"Q-Learning ajanÄ±mÄ±zÄ± **baÅŸlangÄ±Ã§ durumundan (S) hedef durumuna (G) sadece donmuÅŸ karolar (F) Ã¼zerinde yÃ¼rÃ¼yerek ve deliklerden (H)** kaÃ§Ä±narak gitmesi iÃ§in eÄŸiteceÄŸiz.\n\nÄ°ki boyutta Ã§evreye sahip olabiliriz:\n\n- `map_name=\"4x4\"`: 4x4 Ä±zgara versiyonu\n- `map_name=\"8x8\"`: 8x8 Ä±zgara sÃ¼rÃ¼mÃ¼\n\n\nOrtamÄ±n iki modu vardÄ±r:\n\n- `is_slippery=False`: DonmuÅŸ gÃ¶lÃ¼n kaygan olmayan yapÄ±sÄ± nedeniyle ajan her zaman **amaÃ§lanan yÃ¶nde** hareket eder (deterministik).\n- `is_slippery=True`: Temsilci, donmuÅŸ gÃ¶lÃ¼n kaygan doÄŸasÄ± nedeniyle **her zaman amaÃ§lanan yÃ¶nde hareket etmeyebilir** (stokastik).\n\nÅimdilik 4x4 harita ve non-slippery ile basit tutalÄ±m.\nOrtamÄ±n nasÄ±l gÃ¶rselleÅŸtirilmesi gerektiÄŸini belirten `render_mode` adlÄ± bir parametre ekliyoruz. Bizim durumumuzda **sonunda ortamÄ±n bir videosunu kaydetmek istediÄŸimiz iÃ§in, render_mode'u rgb_array** olarak ayarlamamÄ±z gerekiyor.\n\n\"rgb_array\" belgesinde aÃ§Ä±klandÄ±ÄŸÄ± gibi: OrtamÄ±n mevcut durumunu temsil eden tek bir Ã§erÃ§eve dÃ¶ndÃ¼rÃ¼r. Ã‡erÃ§eve, x'e y piksel gÃ¶rÃ¼ntÃ¼ iÃ§in RGB deÄŸerlerini temsil eden (x, y, 3) ÅŸeklinde bir np.ndarray'dir.","metadata":{}},{"cell_type":"code","source":"env = gym.make(\n    'FrozenLake-v1', desc=None, map_name=\"4x4\", \n    is_slippery=False, render_mode=\"rgb_array\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:37.272854Z","iopub.execute_input":"2024-08-13T11:32:37.273222Z","iopub.status.idle":"2024-08-13T11:32:37.287782Z","shell.execute_reply.started":"2024-08-13T11:32:37.273194Z","shell.execute_reply":"2024-08-13T11:32:37.286657Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# bunun gibi kendi Ã¶zel Ä±zgaranÄ±zÄ± oluÅŸturabilirsiniz:\n# desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n# gym.make('FrozenLake-v1', desc=desc, is_slippery=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:29:40.287998Z","iopub.status.idle":"2024-08-13T11:29:40.288396Z","shell.execute_reply.started":"2024-08-13T11:29:40.288208Z","shell.execute_reply":"2024-08-13T11:29:40.288224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:43.808537Z","iopub.execute_input":"2024-08-13T11:32:43.808953Z","iopub.status.idle":"2024-08-13T11:32:43.815767Z","shell.execute_reply.started":"2024-08-13T11:32:43.808921Z","shell.execute_reply":"2024-08-13T11:32:43.814633Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"_____OBSERVATION SPACE_____ \n\nObservation Space Discrete(16)\nSample observation 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"`Observation Space Shape Discrete(16)` ile gÃ¶zlemin **ajanÄ±n mevcut konumunu current_row * ncols + current_col (burada hem satÄ±r hem de col 0'dan baÅŸlar)** olarak temsil eden bir tamsayÄ± olduÄŸunu gÃ¶rÃ¼yoruz.\n\nÃ–rneÄŸin, 4x4 haritasÄ±ndaki hedef konumu aÅŸaÄŸÄ±daki gibi hesaplanabilir: 3 * 4 + 3 = 15. OlasÄ± gÃ¶zlemlerin sayÄ±sÄ± haritanÄ±n boyutuna baÄŸlÄ±dÄ±r. **Ã–rneÄŸin, 4x4 haritada 16 olasÄ± gÃ¶zlem vardÄ±r.**\n\n\nÃ–rneÄŸin, durum = 0 bÃ¶yle gÃ¶rÃ¼nÃ¼r:\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png)","metadata":{}},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:47.408995Z","iopub.execute_input":"2024-08-13T11:32:47.409428Z","iopub.status.idle":"2024-08-13T11:32:47.416455Z","shell.execute_reply.started":"2024-08-13T11:32:47.409392Z","shell.execute_reply":"2024-08-13T11:32:47.415044Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n _____ACTION SPACE_____ \n\nAction Space Shape 4\nAction Space Sample 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Eylem uzayÄ± (temsilcinin gerÃ§ekleÅŸtirebileceÄŸi olasÄ± eylemler kÃ¼mesi) 4 eylem ile ayrÄ±ktÄ±r ğŸ®: \n- 0: SOLA GÄ°T \n- 1: AÅAÄI GÄ°T \n- 2: SAÄA GÄ°T \n- 3: YUKARI GÄ°T\n\nÃ–dÃ¼l fonksiyonu ğŸ’°: \n- Hedefe ulaÅŸma: +1 \n- DeliÄŸe ulaÅŸma: 0 \n- DondurulmuÅŸa ulaÅŸma: 0","metadata":{}},{"cell_type":"markdown","source":"## Create and Initialize the Q-table ğŸ—„ï¸\n\nQ-tablomuzu baÅŸlatma zamanÄ±! KaÃ§ satÄ±r (durum) ve sÃ¼tun (eylem) kullanacaÄŸÄ±mÄ±zÄ± bilmek iÃ§in eylem ve gÃ¶zlem uzayÄ±nÄ± bilmemiz gerekir. DeÄŸerlerini daha Ã¶nceden zaten biliyoruz, ancak algoritmamÄ±zÄ±n farklÄ± ortamlar iÃ§in genelleÅŸtirilebilmesi iÃ§in bunlarÄ± programatik olarak elde etmek isteyeceÄŸiz. Gym bize bunu yapmamÄ±z iÃ§in bir yol sunuyor: **env.action_space.n** ve **env.observation_space.n**","metadata":{}},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:49.892021Z","iopub.execute_input":"2024-08-13T11:32:49.892440Z","iopub.status.idle":"2024-08-13T11:32:49.899150Z","shell.execute_reply.started":"2024-08-13T11:32:49.892405Z","shell.execute_reply":"2024-08-13T11:32:49.897752Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"There are  16  possible states\nThere are  4  possible actions\n","output_type":"stream"}]},{"cell_type":"code","source":"def initialize_q_table(state_space, action_space):\n    Qtable = np.zeros((state_space, action_space))\n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:52.140526Z","iopub.execute_input":"2024-08-13T11:32:52.141591Z","iopub.status.idle":"2024-08-13T11:32:52.146839Z","shell.execute_reply.started":"2024-08-13T11:32:52.141553Z","shell.execute_reply":"2024-08-13T11:32:52.145634Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"Qtable_frozenlake = initialize_q_table(state_space, action_space)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:54.390095Z","iopub.execute_input":"2024-08-13T11:32:54.390826Z","iopub.status.idle":"2024-08-13T11:32:54.395968Z","shell.execute_reply.started":"2024-08-13T11:32:54.390792Z","shell.execute_reply":"2024-08-13T11:32:54.394712Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Define the greedy policy\n\nQ-Learning bir **politika dÄ±ÅŸÄ±** algoritma olduÄŸu iÃ§in iki politikamÄ±z olduÄŸunu unutmayÄ±n. Bu, deÄŸer fonksiyonunu harekete geÃ§irmek ve gÃ¼ncellemek iÃ§in **farklÄ± bir politika** kullandÄ±ÄŸÄ±mÄ±z anlamÄ±na gelir.\n\n- Epsilon-greedy politikasÄ± (hareket politikasÄ±) \n- Greedy-politikasÄ± (gÃ¼ncelleme politikasÄ±)\n\nAÃ§gÃ¶zlÃ¼ politika aynÄ± zamanda Q-Ã¶ÄŸrenme ajanÄ± eÄŸitimi tamamladÄ±ÄŸÄ±nda sahip olacaÄŸÄ±mÄ±z nihai politika olacaktÄ±r. AÃ§gÃ¶zlÃ¼ politika, Q-tablosunu kullanarak bir eylem seÃ§mek iÃ§in kullanÄ±lÄ±r.\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg)","metadata":{}},{"cell_type":"code","source":"def greedy_policy(Qtable, state):\n    action = np.argmax(Qtable[state][:])\n    return action","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:57.916918Z","iopub.execute_input":"2024-08-13T11:32:57.917293Z","iopub.status.idle":"2024-08-13T11:32:57.922703Z","shell.execute_reply.started":"2024-08-13T11:32:57.917263Z","shell.execute_reply":"2024-08-13T11:32:57.921512Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Define the epsilon-greedy policy\n\nEpsilon-greedy, keÅŸif/sÃ¶mÃ¼rÃ¼ deÄŸiÅŸ tokuÅŸunu ele alan eÄŸitim politikasÄ±dÄ±r.\n\nEpsilon-greedy ile ilgili fikir:\n\n- OlasÄ±lÄ±k 1 - É›* ile: **sÃ¶mÃ¼rme** yaparÄ±z (yani ajanÄ±mÄ±z en yÃ¼ksek durum-eylem Ã§ifti deÄŸerine sahip eylemi seÃ§er).\n\n- OlasÄ±lÄ±k É›* ile: **keÅŸif** yaparÄ±z (rastgele bir eylem deneriz).\n\nEÄŸitim devam ettikÃ§e, giderek daha az keÅŸfe ve daha fazla sÃ¶mÃ¼rÃ¼ye ihtiyaÃ§ duyacaÄŸÄ±mÄ±z iÃ§in epsilon deÄŸerini kademeli olarak **azaltÄ±rÄ±z**.\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg)","metadata":{}},{"cell_type":"code","source":"def epsilon_greedy_policy(Qtable, state, epsilon):\n    random_num = random.uniform(0, 1)\n    if random_num > epsilon:\n        action = greedy_policy(Qtable, state)\n    else:\n#         action = random.choice([0, 1, 2, 3])\n        action = env.action_space.sample()\n    \n    return action","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:01.580216Z","iopub.execute_input":"2024-08-13T11:33:01.580635Z","iopub.status.idle":"2024-08-13T11:33:01.586845Z","shell.execute_reply.started":"2024-08-13T11:33:01.580602Z","shell.execute_reply":"2024-08-13T11:33:01.585633Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Define the hyperparameters\n\nKeÅŸifle ilgili hiperparametreler en Ã¶nemlilerinden bazÄ±larÄ±dÄ±r.\n\n- Temsilcimizin iyi bir deÄŸer yaklaÅŸÄ±mÄ± Ã¶ÄŸrenmek iÃ§in **durum uzayÄ±nÄ± yeterince keÅŸfettiÄŸinden** emin olmamÄ±z gerekir. Bunu yapmak iÃ§in, epsilonun aÅŸamalÄ± olarak azalmasÄ± gerekir. \n- Epsilonu Ã§ok hÄ±zlÄ± azaltÄ±rsanÄ±z (Ã§ok yÃ¼ksek decay_rate), **ajanÄ±nÄ±zÄ±n takÄ±lÄ±p kalmasÄ±** riskini alÄ±rsÄ±nÄ±z, Ã§Ã¼nkÃ¼ ajanÄ±nÄ±z durum uzayÄ±nÄ± yeterince keÅŸfetmemiÅŸtir ve bu nedenle sorunu Ã§Ã¶zemez.","metadata":{}},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 10000  # Total training episodes\nlearning_rate = 0.7  # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100  # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"  # Name of the environment\nmax_steps = 99  # Max steps per episode\ngamma = 0.95  # Discounting rate\neval_seed = []  # The evaluation seed of the environment\n\n# Exploration parameters\nmax_epsilon = 1.0  # Exploration probability at start\nmin_epsilon = 0.05  # Minimum exploration probability\ndecay_rate = 0.0005  # Exponential decay rate for exploration prob","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:04.158948Z","iopub.execute_input":"2024-08-13T11:33:04.159350Z","iopub.status.idle":"2024-08-13T11:33:04.165967Z","shell.execute_reply.started":"2024-08-13T11:33:04.159319Z","shell.execute_reply":"2024-08-13T11:33:04.164775Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Create the training loop method\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg)\n\n```\nFor episode in the total of training episodes:\n\nReduce epsilon (since we need less and less exploration)\nReset the environment\n\n  For step in max timesteps:\n    Choose the action At using epsilon greedy policy\n    Take the action (a) and observe the outcome state(s') and reward (r)\n    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n    If done, finish the episode\n    Our next state is the new state\n```","metadata":{}},{"cell_type":"code","source":"def train(n_training_episodes, min_epsilon, max_epsilon, \n          decay_rate, env, max_steps, Qtable):\n    for episode in tqdm(range(n_training_episodes)):\n        # Reduce epsilon (because we need less and less exploration)\n        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n\n        # Reset the environment\n        state, info = env.reset()\n        step = 0\n        termiated = False\n        truncated = False\n\n        for step in range(max_steps):\n            action = epsilon_greedy_policy(Qtable, state, epsilon)\n            new_state, reward, terminated, truncated, info = env.step(action)\n            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n\n            if terminated or truncated:\n                break\n\n            state = new_state\n    \n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:09.576413Z","iopub.execute_input":"2024-08-13T11:33:09.576834Z","iopub.status.idle":"2024-08-13T11:33:09.585825Z","shell.execute_reply.started":"2024-08-13T11:33:09.576799Z","shell.execute_reply":"2024-08-13T11:33:09.584577Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"Qtable_frozenlake = train(\n    n_training_episodes, min_epsilon, max_epsilon, \n    decay_rate, env, max_steps, Qtable_frozenlake\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:12.765476Z","iopub.execute_input":"2024-08-13T11:33:12.765912Z","iopub.status.idle":"2024-08-13T11:33:16.435678Z","shell.execute_reply.started":"2024-08-13T11:33:12.765878Z","shell.execute_reply":"2024-08-13T11:33:16.434547Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cee41f76a564c7b800fcbfc1e0c5fb4"}},"metadata":{}}]},{"cell_type":"code","source":"Qtable_frozenlake","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:16.437742Z","iopub.execute_input":"2024-08-13T11:33:16.438074Z","iopub.status.idle":"2024-08-13T11:33:16.446275Z","shell.execute_reply.started":"2024-08-13T11:33:16.438044Z","shell.execute_reply":"2024-08-13T11:33:16.445240Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n       [0.73509189, 0.        , 0.81450625, 0.77378094],\n       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n       [0.81450625, 0.        , 0.77378092, 0.77378093],\n       [0.77378094, 0.81450625, 0.        , 0.73509189],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.        , 0.81450625],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.81450625, 0.        , 0.857375  , 0.77378094],\n       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n       [0.857375  , 0.95      , 0.        , 0.857375  ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.95      , 0.857375  ],\n       [0.9025    , 0.95      , 1.        , 0.9025    ],\n       [0.        , 0.        , 0.        , 0.        ]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## The evaluation method ğŸ“","metadata":{}},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param Q: The Q-table\n    :param seed: The evaluation seed array (for taxi-v3)\n    \"\"\"\n    episode_rewards = []\n    for episode in tqdm(range(n_eval_episodes)):\n        if seed:\n            state, info = env.reset(seed=seed[episode])\n        else:\n            state, info = env.reset()\n        step = 0\n        truncated = False\n        terminated = False\n        total_rewards_ep = 0\n\n        for step in range(max_steps):\n            # Take the action (index) that have the maximum expected future reward given that state\n            action = greedy_policy(Q, state)\n            new_state, reward, terminated, truncated, info = env.step(action)\n            total_rewards_ep += reward\n\n            if terminated or truncated:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:34:25.547437Z","iopub.execute_input":"2024-08-13T11:34:25.547920Z","iopub.status.idle":"2024-08-13T11:34:25.558239Z","shell.execute_reply.started":"2024-08-13T11:34:25.547888Z","shell.execute_reply":"2024-08-13T11:34:25.557010Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Evaluate our Agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:34:41.826796Z","iopub.execute_input":"2024-08-13T11:34:41.827204Z","iopub.status.idle":"2024-08-13T11:34:41.878802Z","shell.execute_reply.started":"2024-08-13T11:34:41.827169Z","shell.execute_reply":"2024-08-13T11:34:41.877729Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e935314cffe420ba969501671b78620"}},"metadata":{}},{"name":"stdout","text":"Mean_reward=1.00 +/- 0.00\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Publish Trained Model on Hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:36:06.782309Z","iopub.execute_input":"2024-08-13T11:36:06.783089Z","iopub.status.idle":"2024-08-13T11:36:07.229646Z","shell.execute_reply.started":"2024-08-13T11:36:06.783051Z","shell.execute_reply":"2024-08-13T11:36:07.228744Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def record_video(env, Qtable, out_directory, fps=1):\n    \"\"\"\n    Generate a replay video of the agent\n    :param env\n    :param Qtable: Qtable of our agent\n    :param out_directory\n    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n    \"\"\"\n    images = []\n    terminated = False\n    truncated = False\n    state, info = env.reset(seed=random.randint(0, 500))\n    img = env.render()\n    images.append(img)\n    while not terminated or truncated:\n        # Take the action (index) that have the maximum expected future reward given that state\n        action = np.argmax(Qtable[state][:])\n        state, reward, terminated, truncated, info = env.step(\n            action\n        )  # We directly put next_state = state for recording logic\n        img = env.render()\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:36:09.784600Z","iopub.execute_input":"2024-08-13T11:36:09.786065Z","iopub.status.idle":"2024-08-13T11:36:09.795353Z","shell.execute_reply.started":"2024-08-13T11:36:09.786023Z","shell.execute_reply":"2024-08-13T11:36:09.793952Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def push_to_hub(repo_id, model, env, video_fps=1, local_repo_path=\"hub\"):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the Hub\n\n    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n    :param env\n    :param video_fps: how many frame per seconds to record our video replay\n    (with taxi-v3 and frozenlake-v1 we use 1)\n    :param local_repo_path: where the local repository is\n    \"\"\"\n    _, repo_name = repo_id.split(\"/\")\n\n    eval_env = env\n    api = HfApi()\n\n    # Step 1: Create the repo\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n    )\n\n    # Step 2: Download files\n    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n\n    # Step 3: Save the model\n    if env.spec.kwargs.get(\"map_name\"):\n        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n            model[\"slippery\"] = False\n\n    # Pickle the model\n    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n        pickle.dump(model, f)\n\n    # Step 4: Evaluate the model and build JSON with evaluation metrics\n    mean_reward, std_reward = evaluate_agent(\n        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n    )\n\n    evaluate_data = {\n        \"env_id\": model[\"env_id\"],\n        \"mean_reward\": mean_reward,\n        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n        \"eval_datetime\": datetime.datetime.now().isoformat(),\n    }\n\n    # Write a JSON file called \"results.json\" that will contain the\n    # evaluation results\n    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = model[\"env_id\"]\n    if env.spec.kwargs.get(\"map_name\"):\n        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n\n    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n        env_name += \"-\" + \"no_slippery\"\n\n    metadata = {}\n    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Q-Learning** Agent playing1 **{env_id}**\n  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n\n  ## Usage\n\n  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n\n  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n  env = gym.make(model[\"env_id\"])\n  \"\"\"\n\n    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n\n    readme_path = repo_local_path / \"README.md\"\n    readme = \"\"\n    print(readme_path.exists())\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path = repo_local_path / \"replay.mp4\"\n    record_video(env, model[\"qtable\"], video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n        repo_id=repo_id,\n        folder_path=repo_local_path,\n        path_in_repo=\".\",\n    )\n\n    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:37:14.978007Z","iopub.execute_input":"2024-08-13T11:37:14.978417Z","iopub.status.idle":"2024-08-13T11:37:14.998133Z","shell.execute_reply.started":"2024-08-13T11:37:14.978382Z","shell.execute_reply":"2024-08-13T11:37:14.996881Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:37:27.756257Z","iopub.execute_input":"2024-08-13T11:37:27.756664Z","iopub.status.idle":"2024-08-13T11:37:27.790027Z","shell.execute_reply.started":"2024-08-13T11:37:27.756631Z","shell.execute_reply":"2024-08-13T11:37:27.788905Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d98af209a94cbbbe5af16d7a69811c"}},"metadata":{}}]},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n    \"qtable\": Qtable_frozenlake,\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:38:10.931652Z","iopub.execute_input":"2024-08-13T11:38:10.932477Z","iopub.status.idle":"2024-08-13T11:38:10.938542Z","shell.execute_reply.started":"2024-08-13T11:38:10.932440Z","shell.execute_reply":"2024-08-13T11:38:10.937437Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:38:19.892262Z","iopub.execute_input":"2024-08-13T11:38:19.892710Z","iopub.status.idle":"2024-08-13T11:38:19.901742Z","shell.execute_reply.started":"2024-08-13T11:38:19.892656Z","shell.execute_reply":"2024-08-13T11:38:19.900481Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'env_id': 'FrozenLake-v1',\n 'max_steps': 99,\n 'n_training_episodes': 10000,\n 'n_eval_episodes': 100,\n 'eval_seed': [],\n 'learning_rate': 0.7,\n 'gamma': 0.95,\n 'max_epsilon': 1.0,\n 'min_epsilon': 0.05,\n 'decay_rate': 0.0005,\n 'qtable': array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n        [0.73509189, 0.        , 0.81450625, 0.77378094],\n        [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n        [0.81450625, 0.        , 0.77378092, 0.77378093],\n        [0.77378094, 0.81450625, 0.        , 0.73509189],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.9025    , 0.        , 0.81450625],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.81450625, 0.        , 0.857375  , 0.77378094],\n        [0.81450625, 0.9025    , 0.9025    , 0.        ],\n        [0.857375  , 0.95      , 0.        , 0.857375  ],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.9025    , 0.95      , 0.857375  ],\n        [0.9025    , 0.95      , 1.        , 0.9025    ],\n        [0.        , 0.        , 0.        , 0.        ]])}"},"metadata":{}}]},{"cell_type":"code","source":"username = \"Leotrim\"\nrepo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\npush_to_hub(repo_id=f\"{username}/{repo_name}\", model=model, env=env)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:38:39.959749Z","iopub.execute_input":"2024-08-13T11:38:39.960984Z","iopub.status.idle":"2024-08-13T11:38:44.809041Z","shell.execute_reply.started":"2024-08-13T11:38:39.960945Z","shell.execute_reply":"2024-08-13T11:38:44.807710Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bb78786e3b43d9968697ab446a46fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f04e02bebfb4a559b14f60bc451b565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610ef26cd02546b891addc09b888e2b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6708042cab4739b737502fe4b4cc2a"}},"metadata":{}},{"name":"stdout","text":"False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"q-learning.pkl:   0%|          | 0.00/914 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae9eef2592c46d8b52fd4ba7018706d"}},"metadata":{}},{"name":"stdout","text":"Your model is pushed to the Hub. You can view your model here:  https://huggingface.co/Leotrim/q-FrozenLake-v1-4x4-noSlippery\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Part 2: Taxi-v3 ğŸš–\n\nTaxi-v3 ğŸš•'te, Ä±zgara dÃ¼nyasÄ±nda R(ed), G(reen), Y(ellow) ve B(lue) ile gÃ¶sterilen dÃ¶rt belirlenmiÅŸ konum vardÄ±r.\n\nBÃ¶lÃ¼m baÅŸladÄ±ÄŸÄ±nda, taksi rastgele bir karede baÅŸlar ve yolcu rastgele bir konumdadÄ±r. Taksi yolcunun bulunduÄŸu yere gider, yolcuyu alÄ±r, yolcunun varÄ±ÅŸ noktasÄ±na gider (belirlenen dÃ¶rt konumdan biri) ve sonra yolcuyu bÄ±rakÄ±r. Yolcu bÄ±rakÄ±ldÄ±ktan sonra bÃ¶lÃ¼m sona erer.","metadata":{}},{"cell_type":"code","source":"env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:07:42.226631Z","iopub.execute_input":"2024-08-13T12:07:42.227892Z","iopub.status.idle":"2024-08-13T12:07:42.247197Z","shell.execute_reply.started":"2024-08-13T12:07:42.227836Z","shell.execute_reply":"2024-08-13T12:07:42.245983Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:07:50.715762Z","iopub.execute_input":"2024-08-13T12:07:50.716181Z","iopub.status.idle":"2024-08-13T12:07:50.723281Z","shell.execute_reply.started":"2024-08-13T12:07:50.716151Z","shell.execute_reply":"2024-08-13T12:07:50.721866Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"There are  500  possible states\nThere are  6  possible actions\n","output_type":"stream"}]},{"cell_type":"markdown","source":"25 taksi pozisyonu, yolcunun 5 olasÄ± konumu (yolcunun takside olduÄŸu durum dahil) ve 4 varÄ±ÅŸ noktasÄ± olduÄŸu iÃ§in 500 ayrÄ±k durum vardÄ±r.","metadata":{}},{"cell_type":"markdown","source":"Eylem uzayÄ± (temsilcinin gerÃ§ekleÅŸtirebileceÄŸi olasÄ± eylemler kÃ¼mesi) ayrÄ±ktÄ±r ve **6 eylem mevcuttur**:\n\n- 0: gÃ¼neye hareket et\n- 1: kuzeye hareket et\n- 2: doÄŸuya hareket et\n- 3: BatÄ±ya hareket et\n- 4: pikap yolcu\n- 5: Yolcu bÄ±rakma\n\nÃ–dÃ¼l iÅŸlevi:\n\n- BaÅŸka bir Ã¶dÃ¼l tetiklenmediÄŸi sÃ¼rece adÄ±m baÅŸÄ±na -1.\n- +20 yolcu teslim etme.\n- -10 \"alma\" ve \"bÄ±rakma\" eylemlerini yasa dÄ±ÅŸÄ± olarak gerÃ§ekleÅŸtirme.","metadata":{}},{"cell_type":"code","source":"Qtable_taxi = initialize_q_table(state_space, action_space)\nprint(Qtable_taxi)\nprint(\"Q-table shape: \", Qtable_taxi.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:10:45.776849Z","iopub.execute_input":"2024-08-13T12:10:45.777292Z","iopub.status.idle":"2024-08-13T12:10:45.784785Z","shell.execute_reply.started":"2024-08-13T12:10:45.777262Z","shell.execute_reply":"2024-08-13T12:10:45.783671Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n ...\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\nQ-table shape:  (500, 6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define the hyperparameters âš™ï¸","metadata":{}},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 25000  # Total training episodes\nlearning_rate = 0.7  # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100  # Total number of test episodes\n\n# DO NOT MODIFY EVAL_SEED\neval_seed = [\n    16,\n    54,\n    165,\n    177,\n    191,\n    191,\n    120,\n    80,\n    149,\n    178,\n    48,\n    38,\n    6,\n    125,\n    174,\n    73,\n    50,\n    172,\n    100,\n    148,\n    146,\n    6,\n    25,\n    40,\n    68,\n    148,\n    49,\n    167,\n    9,\n    97,\n    164,\n    176,\n    61,\n    7,\n    54,\n    55,\n    161,\n    131,\n    184,\n    51,\n    170,\n    12,\n    120,\n    113,\n    95,\n    126,\n    51,\n    98,\n    36,\n    135,\n    54,\n    82,\n    45,\n    95,\n    89,\n    59,\n    95,\n    124,\n    9,\n    113,\n    58,\n    85,\n    51,\n    134,\n    121,\n    169,\n    105,\n    21,\n    30,\n    11,\n    50,\n    65,\n    12,\n    43,\n    82,\n    145,\n    152,\n    97,\n    106,\n    55,\n    31,\n    85,\n    38,\n    112,\n    102,\n    168,\n    123,\n    97,\n    21,\n    83,\n    158,\n    26,\n    80,\n    63,\n    5,\n    81,\n    32,\n    11,\n    28,\n    148,\n]  # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n# Each seed has a specific starting state\n\n# Environment parameters\nenv_id = \"Taxi-v3\"  # Name of the environment\nmax_steps = 99  # Max steps per episode\ngamma = 0.95  # Discounting rate\n\n# Exploration parameters\nmax_epsilon = 1.0  # Exploration probability at start\nmin_epsilon = 0.05  # Minimum exploration probability\ndecay_rate = 0.005  # Exponential decay rate for exploration prob","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:11:10.482490Z","iopub.execute_input":"2024-08-13T12:11:10.483372Z","iopub.status.idle":"2024-08-13T12:11:10.496855Z","shell.execute_reply.started":"2024-08-13T12:11:10.483333Z","shell.execute_reply":"2024-08-13T12:11:10.495501Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"Qtable_taxi = train(\n    n_training_episodes, min_epsilon, max_epsilon, \n    decay_rate, env, max_steps, Qtable_taxi\n)\nQtable_taxi","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:12:22.307994Z","iopub.execute_input":"2024-08-13T12:12:22.308453Z","iopub.status.idle":"2024-08-13T12:12:46.506799Z","shell.execute_reply.started":"2024-08-13T12:12:22.308421Z","shell.execute_reply":"2024-08-13T12:12:46.505579Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03faab4d09ea4ae3bdc44ae2e561e60c"}},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"array([[  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  2.75200369,   3.94947757,   2.75200369,   3.94947757,\n          5.20997639,  -5.05052243],\n       [  7.93349125,   9.40367562,   5.2099758 ,   9.40365143,\n         10.9512375 ,   0.40367562],\n       ...,\n       [ 10.9512373 ,  12.58025   ,  10.9512375 ,   9.40367561,\n          1.95123746,   1.9512375 ],\n       [ -4.42194461,   6.53681725,   4.35298331,  -4.35891922,\n        -13.22160565, -12.14572498],\n       [ -0.91      ,  -0.973     ,  11.00822128,  18.        ,\n          4.91822128,   0.95421971]])"},"metadata":{}}]},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n    \"qtable\": Qtable_taxi,\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:12:52.558679Z","iopub.execute_input":"2024-08-13T12:12:52.559117Z","iopub.status.idle":"2024-08-13T12:12:52.566011Z","shell.execute_reply.started":"2024-08-13T12:12:52.559083Z","shell.execute_reply":"2024-08-13T12:12:52.564603Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"username = \"Leotrim\"\nrepo_name = \"Taxi-v3\"\npush_to_hub(repo_id=f\"{username}/{repo_name}\", model=model, env=env)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:13:34.124040Z","iopub.execute_input":"2024-08-13T12:13:34.124804Z","iopub.status.idle":"2024-08-13T12:13:38.419964Z","shell.execute_reply.started":"2024-08-13T12:13:34.124763Z","shell.execute_reply":"2024-08-13T12:13:38.418565Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e8b9c3d3ae44509f64cb04fcaff294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c6eb990748e406590bb9e1f36bdc443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe24a7a6aa846098739f88a4e2bcb37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28ace3707a9f4f219c2b28788e58e9bf"}},"metadata":{}},{"name":"stdout","text":"False\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x64903c0] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"q-learning.pkl:   0%|          | 0.00/24.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c694200cc3a4675b0e8feb33680cf9d"}},"metadata":{}},{"name":"stdout","text":"Your model is pushed to the Hub. You can view your model here:  https://huggingface.co/Leotrim/Taxi-v3\n","output_type":"stream"}]}]}