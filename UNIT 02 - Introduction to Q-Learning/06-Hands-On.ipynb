{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hands-on\n\nQ-Learning algoritmasını incelediğimize göre, şimdi onu sıfırdan uygulayalım ve Q-Learning ajanımızı iki ortamda eğitelim:\n\n1. Frozen-Lake-v1 (kaygan olmayan ve kaygan versiyon) ☃️ : burada ajanımızın başlangıç durumundan (S) hedef durumuna (G) sadece donmuş karolar (F) üzerinde yürüyerek ve deliklerden (H) kaçınarak gitmesi gerekecektir.\n\n2. Otonom bir taksinin 🚖 yolcularını A noktasından B noktasına taşımak için bir şehirde gezinmeyi öğrenmesi gerekecektir.\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Unit 2: Q-Learning with FrozenLake-v1 ⛄ and Taxi-v3 🚕\n\n### 🎮 Environments:\n\n- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n\n### 📚 RL-Library:\n\n- Python and NumPy\n- [Gymnasium](https://gymnasium.farama.org/)\n\n## Objectives of this notebook 🏆\n\nNot defterinin sonunda, şunları yapacaksınız:\n\n- Ortam kütüphanesi olan **Gymnasium**'u kullanabilecek \n- Sıfırdan bir Q-Learning ajanını kodlayabilecek \n- Eğitilmiş ajanınızı ve kodunuzu güzel bir video tekrarı ve bir değerlendirme puanı ile **Hub'a** gönderebileceksiniz 🔥.","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:30:48.617643Z","iopub.execute_input":"2024-08-13T11:30:48.618080Z","iopub.status.idle":"2024-08-13T11:31:15.862969Z","shell.execute_reply.started":"2024-08-13T11:30:48.618045Z","shell.execute_reply":"2024-08-13T11:31:15.861622Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (0.29.0)\nCollecting pygame (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 2))\n  Downloading pygame-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 3)) (1.26.4)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (0.23.4)\nCollecting pickle5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 6))\n  Downloading pickle5-0.0.11.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pyyaml==6.0 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 7))\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (2.33.1)\nCollecting imageio_ffmpeg (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9))\n  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\nCollecting pyglet==1.5.1 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 10))\n  Downloading pyglet-1.5.1-py2.py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 11)) (4.66.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (2.2.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (4.9.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (0.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2.32.3)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (9.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio_ffmpeg->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2024.7.4)\nDownloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyglet-1.5.1-py2.py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pygame-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pickle5\n  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=125214 sha256=52c9d7bb57d1d17187d889abd319ce2b5c147d071433edeff23922fcc5f27b73\n  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\nSuccessfully built pickle5\nInstalling collected packages: pyglet, pickle5, pyyaml, pygame, imageio_ffmpeg\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.1\n    Uninstalling PyYAML-6.0.1:\n      Successfully uninstalled PyYAML-6.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imageio_ffmpeg-0.5.1 pickle5-0.0.11 pygame-2.6.0 pyglet-1.5.1 pyyaml-6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install -y python3-opengl\n!apt install ffmpeg xvfb\n!pip3 install -q pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:31:15.864885Z","iopub.execute_input":"2024-08-13T11:31:15.865200Z","iopub.status.idle":"2024-08-13T11:31:52.300487Z","shell.execute_reply.started":"2024-08-13T11:31:15.865167Z","shell.execute_reply":"2024-08-13T11:31:52.298950Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Get:1 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]\nGet:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [1616 B]       \nGet:3 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]      \nHit:4 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:5 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [25.6 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:7 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3160 kB]\nGet:8 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1496 kB]\nGet:9 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1256 kB]\nHit:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4018 kB]\nGet:12 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3867 kB]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3885 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4351 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1543 kB]\nFetched 23.9 MB in 4s (6129 kB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  freeglut3 libglu1-mesa\nSuggested packages:\n  python3-numpy libgle3\nThe following NEW packages will be installed:\n  freeglut3 libglu1-mesa python3-opengl\n0 upgraded, 3 newly installed, 0 to remove and 86 not upgraded.\nNeed to get 728 kB of archives.\nAfter this operation, 6216 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libglu1-mesa amd64 9.0.1-1build1 [168 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python3-opengl all 3.1.0+dfsg-2build1 [486 kB]\nFetched 728 kB in 2s (379 kB/s)        \nSelecting previously unselected package freeglut3:amd64.\n(Reading database ... 110195 files and directories currently installed.)\nPreparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\nUnpacking freeglut3:amd64 (2.8.1-3) ...\nSelecting previously unselected package libglu1-mesa:amd64.\nPreparing to unpack .../libglu1-mesa_9.0.1-1build1_amd64.deb ...\nUnpacking libglu1-mesa:amd64 (9.0.1-1build1) ...\nSelecting previously unselected package python3-opengl.\nPreparing to unpack .../python3-opengl_3.1.0+dfsg-2build1_all.deb ...\nUnpacking python3-opengl (3.1.0+dfsg-2build1) ...\nSetting up freeglut3:amd64 (2.8.1-3) ...\nSetting up libglu1-mesa:amd64 (9.0.1-1build1) ...\nSetting up python3-opengl (3.1.0+dfsg-2build1) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.14) ...\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\nxvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.17).\n0 upgraded, 0 newly installed, 0 to remove and 86 not upgraded.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Yeni yüklenen kütüphanelerin kullanıldığından emin olmak için bazen notebook çalışma zamanını yeniden başlatmak gerekir. Bir sonraki hücre çalışma zamanını çökmeye zorlayacaktır, bu nedenle tekrar bağlanmanız ve kodu buradan başlayarak çalıştırmanız gerekecektir. Bu hile sayesinde sanal ekranımızı çalıştırabileceğiz.","metadata":{}},{"cell_type":"code","source":"import os\n\nos.kill(os.getpid(), 9)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:20:48.951063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:31:52.302939Z","iopub.execute_input":"2024-08-13T11:31:52.303419Z","iopub.status.idle":"2024-08-13T11:31:52.715523Z","shell.execute_reply.started":"2024-08-13T11:31:52.303379Z","shell.execute_reply":"2024-08-13T11:31:52.714384Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<pyvirtualdisplay.display.Display at 0x78996cdbdb40>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Importing Necessary Packages","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport gymnasium as gym\nimport random\nimport imageio\nimport tqdm\nimport pickle5 as pickle\n\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:32.554187Z","iopub.execute_input":"2024-08-13T11:32:32.554607Z","iopub.status.idle":"2024-08-13T11:32:33.308246Z","shell.execute_reply.started":"2024-08-13T11:32:32.554574Z","shell.execute_reply":"2024-08-13T11:32:33.306778Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: Frozen Lake ⛄ (non slippery version)","metadata":{}},{"cell_type":"markdown","source":"Q-Learning ajanımızı **başlangıç durumundan (S) hedef durumuna (G) sadece donmuş karolar (F) üzerinde yürüyerek ve deliklerden (H)** kaçınarak gitmesi için eğiteceğiz.\n\nİki boyutta çevreye sahip olabiliriz:\n\n- `map_name=\"4x4\"`: 4x4 ızgara versiyonu\n- `map_name=\"8x8\"`: 8x8 ızgara sürümü\n\n\nOrtamın iki modu vardır:\n\n- `is_slippery=False`: Donmuş gölün kaygan olmayan yapısı nedeniyle ajan her zaman **amaçlanan yönde** hareket eder (deterministik).\n- `is_slippery=True`: Temsilci, donmuş gölün kaygan doğası nedeniyle **her zaman amaçlanan yönde hareket etmeyebilir** (stokastik).\n\nŞimdilik 4x4 harita ve non-slippery ile basit tutalım.\nOrtamın nasıl görselleştirilmesi gerektiğini belirten `render_mode` adlı bir parametre ekliyoruz. Bizim durumumuzda **sonunda ortamın bir videosunu kaydetmek istediğimiz için, render_mode'u rgb_array** olarak ayarlamamız gerekiyor.\n\n\"rgb_array\" belgesinde açıklandığı gibi: Ortamın mevcut durumunu temsil eden tek bir çerçeve döndürür. Çerçeve, x'e y piksel görüntü için RGB değerlerini temsil eden (x, y, 3) şeklinde bir np.ndarray'dir.","metadata":{}},{"cell_type":"code","source":"env = gym.make(\n    'FrozenLake-v1', desc=None, map_name=\"4x4\", \n    is_slippery=False, render_mode=\"rgb_array\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:37.272854Z","iopub.execute_input":"2024-08-13T11:32:37.273222Z","iopub.status.idle":"2024-08-13T11:32:37.287782Z","shell.execute_reply.started":"2024-08-13T11:32:37.273194Z","shell.execute_reply":"2024-08-13T11:32:37.286657Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# bunun gibi kendi özel ızgaranızı oluşturabilirsiniz:\n# desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n# gym.make('FrozenLake-v1', desc=desc, is_slippery=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:29:40.287998Z","iopub.status.idle":"2024-08-13T11:29:40.288396Z","shell.execute_reply.started":"2024-08-13T11:29:40.288208Z","shell.execute_reply":"2024-08-13T11:29:40.288224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space\", env.observation_space)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:43.808537Z","iopub.execute_input":"2024-08-13T11:32:43.808953Z","iopub.status.idle":"2024-08-13T11:32:43.815767Z","shell.execute_reply.started":"2024-08-13T11:32:43.808921Z","shell.execute_reply":"2024-08-13T11:32:43.814633Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"_____OBSERVATION SPACE_____ \n\nObservation Space Discrete(16)\nSample observation 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"`Observation Space Shape Discrete(16)` ile gözlemin **ajanın mevcut konumunu current_row * ncols + current_col (burada hem satır hem de col 0'dan başlar)** olarak temsil eden bir tamsayı olduğunu görüyoruz.\n\nÖrneğin, 4x4 haritasındaki hedef konumu aşağıdaki gibi hesaplanabilir: 3 * 4 + 3 = 15. Olası gözlemlerin sayısı haritanın boyutuna bağlıdır. **Örneğin, 4x4 haritada 16 olası gözlem vardır.**\n\n\nÖrneğin, durum = 0 böyle görünür:\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png)","metadata":{}},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:47.408995Z","iopub.execute_input":"2024-08-13T11:32:47.409428Z","iopub.status.idle":"2024-08-13T11:32:47.416455Z","shell.execute_reply.started":"2024-08-13T11:32:47.409392Z","shell.execute_reply":"2024-08-13T11:32:47.415044Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n _____ACTION SPACE_____ \n\nAction Space Shape 4\nAction Space Sample 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Eylem uzayı (temsilcinin gerçekleştirebileceği olası eylemler kümesi) 4 eylem ile ayrıktır 🎮: \n- 0: SOLA GİT \n- 1: AŞAĞI GİT \n- 2: SAĞA GİT \n- 3: YUKARI GİT\n\nÖdül fonksiyonu 💰: \n- Hedefe ulaşma: +1 \n- Deliğe ulaşma: 0 \n- Dondurulmuşa ulaşma: 0","metadata":{}},{"cell_type":"markdown","source":"## Create and Initialize the Q-table 🗄️\n\nQ-tablomuzu başlatma zamanı! Kaç satır (durum) ve sütun (eylem) kullanacağımızı bilmek için eylem ve gözlem uzayını bilmemiz gerekir. Değerlerini daha önceden zaten biliyoruz, ancak algoritmamızın farklı ortamlar için genelleştirilebilmesi için bunları programatik olarak elde etmek isteyeceğiz. Gym bize bunu yapmamız için bir yol sunuyor: **env.action_space.n** ve **env.observation_space.n**","metadata":{}},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:49.892021Z","iopub.execute_input":"2024-08-13T11:32:49.892440Z","iopub.status.idle":"2024-08-13T11:32:49.899150Z","shell.execute_reply.started":"2024-08-13T11:32:49.892405Z","shell.execute_reply":"2024-08-13T11:32:49.897752Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"There are  16  possible states\nThere are  4  possible actions\n","output_type":"stream"}]},{"cell_type":"code","source":"def initialize_q_table(state_space, action_space):\n    Qtable = np.zeros((state_space, action_space))\n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:52.140526Z","iopub.execute_input":"2024-08-13T11:32:52.141591Z","iopub.status.idle":"2024-08-13T11:32:52.146839Z","shell.execute_reply.started":"2024-08-13T11:32:52.141553Z","shell.execute_reply":"2024-08-13T11:32:52.145634Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"Qtable_frozenlake = initialize_q_table(state_space, action_space)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:54.390095Z","iopub.execute_input":"2024-08-13T11:32:54.390826Z","iopub.status.idle":"2024-08-13T11:32:54.395968Z","shell.execute_reply.started":"2024-08-13T11:32:54.390792Z","shell.execute_reply":"2024-08-13T11:32:54.394712Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Define the greedy policy\n\nQ-Learning bir **politika dışı** algoritma olduğu için iki politikamız olduğunu unutmayın. Bu, değer fonksiyonunu harekete geçirmek ve güncellemek için **farklı bir politika** kullandığımız anlamına gelir.\n\n- Epsilon-greedy politikası (hareket politikası) \n- Greedy-politikası (güncelleme politikası)\n\nAçgözlü politika aynı zamanda Q-öğrenme ajanı eğitimi tamamladığında sahip olacağımız nihai politika olacaktır. Açgözlü politika, Q-tablosunu kullanarak bir eylem seçmek için kullanılır.\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg)","metadata":{}},{"cell_type":"code","source":"def greedy_policy(Qtable, state):\n    action = np.argmax(Qtable[state][:])\n    return action","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:32:57.916918Z","iopub.execute_input":"2024-08-13T11:32:57.917293Z","iopub.status.idle":"2024-08-13T11:32:57.922703Z","shell.execute_reply.started":"2024-08-13T11:32:57.917263Z","shell.execute_reply":"2024-08-13T11:32:57.921512Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Define the epsilon-greedy policy\n\nEpsilon-greedy, keşif/sömürü değiş tokuşunu ele alan eğitim politikasıdır.\n\nEpsilon-greedy ile ilgili fikir:\n\n- Olasılık 1 - ɛ* ile: **sömürme** yaparız (yani ajanımız en yüksek durum-eylem çifti değerine sahip eylemi seçer).\n\n- Olasılık ɛ* ile: **keşif** yaparız (rastgele bir eylem deneriz).\n\nEğitim devam ettikçe, giderek daha az keşfe ve daha fazla sömürüye ihtiyaç duyacağımız için epsilon değerini kademeli olarak **azaltırız**.\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg)","metadata":{}},{"cell_type":"code","source":"def epsilon_greedy_policy(Qtable, state, epsilon):\n    random_num = random.uniform(0, 1)\n    if random_num > epsilon:\n        action = greedy_policy(Qtable, state)\n    else:\n#         action = random.choice([0, 1, 2, 3])\n        action = env.action_space.sample()\n    \n    return action","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:01.580216Z","iopub.execute_input":"2024-08-13T11:33:01.580635Z","iopub.status.idle":"2024-08-13T11:33:01.586845Z","shell.execute_reply.started":"2024-08-13T11:33:01.580602Z","shell.execute_reply":"2024-08-13T11:33:01.585633Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Define the hyperparameters\n\nKeşifle ilgili hiperparametreler en önemlilerinden bazılarıdır.\n\n- Temsilcimizin iyi bir değer yaklaşımı öğrenmek için **durum uzayını yeterince keşfettiğinden** emin olmamız gerekir. Bunu yapmak için, epsilonun aşamalı olarak azalması gerekir. \n- Epsilonu çok hızlı azaltırsanız (çok yüksek decay_rate), **ajanınızın takılıp kalması** riskini alırsınız, çünkü ajanınız durum uzayını yeterince keşfetmemiştir ve bu nedenle sorunu çözemez.","metadata":{}},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 10000  # Total training episodes\nlearning_rate = 0.7  # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100  # Total number of test episodes\n\n# Environment parameters\nenv_id = \"FrozenLake-v1\"  # Name of the environment\nmax_steps = 99  # Max steps per episode\ngamma = 0.95  # Discounting rate\neval_seed = []  # The evaluation seed of the environment\n\n# Exploration parameters\nmax_epsilon = 1.0  # Exploration probability at start\nmin_epsilon = 0.05  # Minimum exploration probability\ndecay_rate = 0.0005  # Exponential decay rate for exploration prob","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:04.158948Z","iopub.execute_input":"2024-08-13T11:33:04.159350Z","iopub.status.idle":"2024-08-13T11:33:04.165967Z","shell.execute_reply.started":"2024-08-13T11:33:04.159319Z","shell.execute_reply":"2024-08-13T11:33:04.164775Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Create the training loop method\n\n![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg)\n\n```\nFor episode in the total of training episodes:\n\nReduce epsilon (since we need less and less exploration)\nReset the environment\n\n  For step in max timesteps:\n    Choose the action At using epsilon greedy policy\n    Take the action (a) and observe the outcome state(s') and reward (r)\n    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n    If done, finish the episode\n    Our next state is the new state\n```","metadata":{}},{"cell_type":"code","source":"def train(n_training_episodes, min_epsilon, max_epsilon, \n          decay_rate, env, max_steps, Qtable):\n    for episode in tqdm(range(n_training_episodes)):\n        # Reduce epsilon (because we need less and less exploration)\n        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n\n        # Reset the environment\n        state, info = env.reset()\n        step = 0\n        termiated = False\n        truncated = False\n\n        for step in range(max_steps):\n            action = epsilon_greedy_policy(Qtable, state, epsilon)\n            new_state, reward, terminated, truncated, info = env.step(action)\n            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n\n            if terminated or truncated:\n                break\n\n            state = new_state\n    \n    return Qtable","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:09.576413Z","iopub.execute_input":"2024-08-13T11:33:09.576834Z","iopub.status.idle":"2024-08-13T11:33:09.585825Z","shell.execute_reply.started":"2024-08-13T11:33:09.576799Z","shell.execute_reply":"2024-08-13T11:33:09.584577Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"Qtable_frozenlake = train(\n    n_training_episodes, min_epsilon, max_epsilon, \n    decay_rate, env, max_steps, Qtable_frozenlake\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:12.765476Z","iopub.execute_input":"2024-08-13T11:33:12.765912Z","iopub.status.idle":"2024-08-13T11:33:16.435678Z","shell.execute_reply.started":"2024-08-13T11:33:12.765878Z","shell.execute_reply":"2024-08-13T11:33:16.434547Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cee41f76a564c7b800fcbfc1e0c5fb4"}},"metadata":{}}]},{"cell_type":"code","source":"Qtable_frozenlake","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:33:16.437742Z","iopub.execute_input":"2024-08-13T11:33:16.438074Z","iopub.status.idle":"2024-08-13T11:33:16.446275Z","shell.execute_reply.started":"2024-08-13T11:33:16.438044Z","shell.execute_reply":"2024-08-13T11:33:16.445240Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n       [0.73509189, 0.        , 0.81450625, 0.77378094],\n       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n       [0.81450625, 0.        , 0.77378092, 0.77378093],\n       [0.77378094, 0.81450625, 0.        , 0.73509189],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.        , 0.81450625],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.81450625, 0.        , 0.857375  , 0.77378094],\n       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n       [0.857375  , 0.95      , 0.        , 0.857375  ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.95      , 0.857375  ],\n       [0.9025    , 0.95      , 1.        , 0.9025    ],\n       [0.        , 0.        , 0.        , 0.        ]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## The evaluation method 📝","metadata":{}},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param Q: The Q-table\n    :param seed: The evaluation seed array (for taxi-v3)\n    \"\"\"\n    episode_rewards = []\n    for episode in tqdm(range(n_eval_episodes)):\n        if seed:\n            state, info = env.reset(seed=seed[episode])\n        else:\n            state, info = env.reset()\n        step = 0\n        truncated = False\n        terminated = False\n        total_rewards_ep = 0\n\n        for step in range(max_steps):\n            # Take the action (index) that have the maximum expected future reward given that state\n            action = greedy_policy(Q, state)\n            new_state, reward, terminated, truncated, info = env.step(action)\n            total_rewards_ep += reward\n\n            if terminated or truncated:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:34:25.547437Z","iopub.execute_input":"2024-08-13T11:34:25.547920Z","iopub.status.idle":"2024-08-13T11:34:25.558239Z","shell.execute_reply.started":"2024-08-13T11:34:25.547888Z","shell.execute_reply":"2024-08-13T11:34:25.557010Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Evaluate our Agent\nmean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\nprint(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:34:41.826796Z","iopub.execute_input":"2024-08-13T11:34:41.827204Z","iopub.status.idle":"2024-08-13T11:34:41.878802Z","shell.execute_reply.started":"2024-08-13T11:34:41.827169Z","shell.execute_reply":"2024-08-13T11:34:41.877729Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e935314cffe420ba969501671b78620"}},"metadata":{}},{"name":"stdout","text":"Mean_reward=1.00 +/- 0.00\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Publish Trained Model on Hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:36:06.782309Z","iopub.execute_input":"2024-08-13T11:36:06.783089Z","iopub.status.idle":"2024-08-13T11:36:07.229646Z","shell.execute_reply.started":"2024-08-13T11:36:06.783051Z","shell.execute_reply":"2024-08-13T11:36:07.228744Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def record_video(env, Qtable, out_directory, fps=1):\n    \"\"\"\n    Generate a replay video of the agent\n    :param env\n    :param Qtable: Qtable of our agent\n    :param out_directory\n    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n    \"\"\"\n    images = []\n    terminated = False\n    truncated = False\n    state, info = env.reset(seed=random.randint(0, 500))\n    img = env.render()\n    images.append(img)\n    while not terminated or truncated:\n        # Take the action (index) that have the maximum expected future reward given that state\n        action = np.argmax(Qtable[state][:])\n        state, reward, terminated, truncated, info = env.step(\n            action\n        )  # We directly put next_state = state for recording logic\n        img = env.render()\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:36:09.784600Z","iopub.execute_input":"2024-08-13T11:36:09.786065Z","iopub.status.idle":"2024-08-13T11:36:09.795353Z","shell.execute_reply.started":"2024-08-13T11:36:09.786023Z","shell.execute_reply":"2024-08-13T11:36:09.793952Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def push_to_hub(repo_id, model, env, video_fps=1, local_repo_path=\"hub\"):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the Hub\n\n    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n    :param env\n    :param video_fps: how many frame per seconds to record our video replay\n    (with taxi-v3 and frozenlake-v1 we use 1)\n    :param local_repo_path: where the local repository is\n    \"\"\"\n    _, repo_name = repo_id.split(\"/\")\n\n    eval_env = env\n    api = HfApi()\n\n    # Step 1: Create the repo\n    repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n    )\n\n    # Step 2: Download files\n    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n\n    # Step 3: Save the model\n    if env.spec.kwargs.get(\"map_name\"):\n        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n            model[\"slippery\"] = False\n\n    # Pickle the model\n    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n        pickle.dump(model, f)\n\n    # Step 4: Evaluate the model and build JSON with evaluation metrics\n    mean_reward, std_reward = evaluate_agent(\n        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n    )\n\n    evaluate_data = {\n        \"env_id\": model[\"env_id\"],\n        \"mean_reward\": mean_reward,\n        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n        \"eval_datetime\": datetime.datetime.now().isoformat(),\n    }\n\n    # Write a JSON file called \"results.json\" that will contain the\n    # evaluation results\n    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = model[\"env_id\"]\n    if env.spec.kwargs.get(\"map_name\"):\n        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n\n    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n        env_name += \"-\" + \"no_slippery\"\n\n    metadata = {}\n    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Q-Learning** Agent playing1 **{env_id}**\n  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n\n  ## Usage\n\n  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n\n  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n  env = gym.make(model[\"env_id\"])\n  \"\"\"\n\n    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n\n    readme_path = repo_local_path / \"README.md\"\n    readme = \"\"\n    print(readme_path.exists())\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path = repo_local_path / \"replay.mp4\"\n    record_video(env, model[\"qtable\"], video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n        repo_id=repo_id,\n        folder_path=repo_local_path,\n        path_in_repo=\".\",\n    )\n\n    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:37:14.978007Z","iopub.execute_input":"2024-08-13T11:37:14.978417Z","iopub.status.idle":"2024-08-13T11:37:14.998133Z","shell.execute_reply.started":"2024-08-13T11:37:14.978382Z","shell.execute_reply":"2024-08-13T11:37:14.996881Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:37:27.756257Z","iopub.execute_input":"2024-08-13T11:37:27.756664Z","iopub.status.idle":"2024-08-13T11:37:27.790027Z","shell.execute_reply.started":"2024-08-13T11:37:27.756631Z","shell.execute_reply":"2024-08-13T11:37:27.788905Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d98af209a94cbbbe5af16d7a69811c"}},"metadata":{}}]},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n    \"qtable\": Qtable_frozenlake,\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:38:10.931652Z","iopub.execute_input":"2024-08-13T11:38:10.932477Z","iopub.status.idle":"2024-08-13T11:38:10.938542Z","shell.execute_reply.started":"2024-08-13T11:38:10.932440Z","shell.execute_reply":"2024-08-13T11:38:10.937437Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:38:19.892262Z","iopub.execute_input":"2024-08-13T11:38:19.892710Z","iopub.status.idle":"2024-08-13T11:38:19.901742Z","shell.execute_reply.started":"2024-08-13T11:38:19.892656Z","shell.execute_reply":"2024-08-13T11:38:19.900481Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'env_id': 'FrozenLake-v1',\n 'max_steps': 99,\n 'n_training_episodes': 10000,\n 'n_eval_episodes': 100,\n 'eval_seed': [],\n 'learning_rate': 0.7,\n 'gamma': 0.95,\n 'max_epsilon': 1.0,\n 'min_epsilon': 0.05,\n 'decay_rate': 0.0005,\n 'qtable': array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n        [0.73509189, 0.        , 0.81450625, 0.77378094],\n        [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n        [0.81450625, 0.        , 0.77378092, 0.77378093],\n        [0.77378094, 0.81450625, 0.        , 0.73509189],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.9025    , 0.        , 0.81450625],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.81450625, 0.        , 0.857375  , 0.77378094],\n        [0.81450625, 0.9025    , 0.9025    , 0.        ],\n        [0.857375  , 0.95      , 0.        , 0.857375  ],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.        ],\n        [0.        , 0.9025    , 0.95      , 0.857375  ],\n        [0.9025    , 0.95      , 1.        , 0.9025    ],\n        [0.        , 0.        , 0.        , 0.        ]])}"},"metadata":{}}]},{"cell_type":"code","source":"username = \"Leotrim\"\nrepo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\npush_to_hub(repo_id=f\"{username}/{repo_name}\", model=model, env=env)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:38:39.959749Z","iopub.execute_input":"2024-08-13T11:38:39.960984Z","iopub.status.idle":"2024-08-13T11:38:44.809041Z","shell.execute_reply.started":"2024-08-13T11:38:39.960945Z","shell.execute_reply":"2024-08-13T11:38:44.807710Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bb78786e3b43d9968697ab446a46fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f04e02bebfb4a559b14f60bc451b565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610ef26cd02546b891addc09b888e2b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6708042cab4739b737502fe4b4cc2a"}},"metadata":{}},{"name":"stdout","text":"False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"q-learning.pkl:   0%|          | 0.00/914 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae9eef2592c46d8b52fd4ba7018706d"}},"metadata":{}},{"name":"stdout","text":"Your model is pushed to the Hub. You can view your model here:  https://huggingface.co/Leotrim/q-FrozenLake-v1-4x4-noSlippery\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Part 2: Taxi-v3 🚖\n\nTaxi-v3 🚕'te, ızgara dünyasında R(ed), G(reen), Y(ellow) ve B(lue) ile gösterilen dört belirlenmiş konum vardır.\n\nBölüm başladığında, taksi rastgele bir karede başlar ve yolcu rastgele bir konumdadır. Taksi yolcunun bulunduğu yere gider, yolcuyu alır, yolcunun varış noktasına gider (belirlenen dört konumdan biri) ve sonra yolcuyu bırakır. Yolcu bırakıldıktan sonra bölüm sona erer.","metadata":{}},{"cell_type":"code","source":"env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:07:42.226631Z","iopub.execute_input":"2024-08-13T12:07:42.227892Z","iopub.status.idle":"2024-08-13T12:07:42.247197Z","shell.execute_reply.started":"2024-08-13T12:07:42.227836Z","shell.execute_reply":"2024-08-13T12:07:42.245983Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"state_space = env.observation_space.n\nprint(\"There are \", state_space, \" possible states\")\n\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible actions\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:07:50.715762Z","iopub.execute_input":"2024-08-13T12:07:50.716181Z","iopub.status.idle":"2024-08-13T12:07:50.723281Z","shell.execute_reply.started":"2024-08-13T12:07:50.716151Z","shell.execute_reply":"2024-08-13T12:07:50.721866Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"There are  500  possible states\nThere are  6  possible actions\n","output_type":"stream"}]},{"cell_type":"markdown","source":"25 taksi pozisyonu, yolcunun 5 olası konumu (yolcunun takside olduğu durum dahil) ve 4 varış noktası olduğu için 500 ayrık durum vardır.","metadata":{}},{"cell_type":"markdown","source":"Eylem uzayı (temsilcinin gerçekleştirebileceği olası eylemler kümesi) ayrıktır ve **6 eylem mevcuttur**:\n\n- 0: güneye hareket et\n- 1: kuzeye hareket et\n- 2: doğuya hareket et\n- 3: Batıya hareket et\n- 4: pikap yolcu\n- 5: Yolcu bırakma\n\nÖdül işlevi:\n\n- Başka bir ödül tetiklenmediği sürece adım başına -1.\n- +20 yolcu teslim etme.\n- -10 \"alma\" ve \"bırakma\" eylemlerini yasa dışı olarak gerçekleştirme.","metadata":{}},{"cell_type":"code","source":"Qtable_taxi = initialize_q_table(state_space, action_space)\nprint(Qtable_taxi)\nprint(\"Q-table shape: \", Qtable_taxi.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:10:45.776849Z","iopub.execute_input":"2024-08-13T12:10:45.777292Z","iopub.status.idle":"2024-08-13T12:10:45.784785Z","shell.execute_reply.started":"2024-08-13T12:10:45.777262Z","shell.execute_reply":"2024-08-13T12:10:45.783671Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n ...\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\nQ-table shape:  (500, 6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define the hyperparameters ⚙️","metadata":{}},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 25000  # Total training episodes\nlearning_rate = 0.7  # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100  # Total number of test episodes\n\n# DO NOT MODIFY EVAL_SEED\neval_seed = [\n    16,\n    54,\n    165,\n    177,\n    191,\n    191,\n    120,\n    80,\n    149,\n    178,\n    48,\n    38,\n    6,\n    125,\n    174,\n    73,\n    50,\n    172,\n    100,\n    148,\n    146,\n    6,\n    25,\n    40,\n    68,\n    148,\n    49,\n    167,\n    9,\n    97,\n    164,\n    176,\n    61,\n    7,\n    54,\n    55,\n    161,\n    131,\n    184,\n    51,\n    170,\n    12,\n    120,\n    113,\n    95,\n    126,\n    51,\n    98,\n    36,\n    135,\n    54,\n    82,\n    45,\n    95,\n    89,\n    59,\n    95,\n    124,\n    9,\n    113,\n    58,\n    85,\n    51,\n    134,\n    121,\n    169,\n    105,\n    21,\n    30,\n    11,\n    50,\n    65,\n    12,\n    43,\n    82,\n    145,\n    152,\n    97,\n    106,\n    55,\n    31,\n    85,\n    38,\n    112,\n    102,\n    168,\n    123,\n    97,\n    21,\n    83,\n    158,\n    26,\n    80,\n    63,\n    5,\n    81,\n    32,\n    11,\n    28,\n    148,\n]  # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n# Each seed has a specific starting state\n\n# Environment parameters\nenv_id = \"Taxi-v3\"  # Name of the environment\nmax_steps = 99  # Max steps per episode\ngamma = 0.95  # Discounting rate\n\n# Exploration parameters\nmax_epsilon = 1.0  # Exploration probability at start\nmin_epsilon = 0.05  # Minimum exploration probability\ndecay_rate = 0.005  # Exponential decay rate for exploration prob","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:11:10.482490Z","iopub.execute_input":"2024-08-13T12:11:10.483372Z","iopub.status.idle":"2024-08-13T12:11:10.496855Z","shell.execute_reply.started":"2024-08-13T12:11:10.483333Z","shell.execute_reply":"2024-08-13T12:11:10.495501Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"Qtable_taxi = train(\n    n_training_episodes, min_epsilon, max_epsilon, \n    decay_rate, env, max_steps, Qtable_taxi\n)\nQtable_taxi","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:12:22.307994Z","iopub.execute_input":"2024-08-13T12:12:22.308453Z","iopub.status.idle":"2024-08-13T12:12:46.506799Z","shell.execute_reply.started":"2024-08-13T12:12:22.308421Z","shell.execute_reply":"2024-08-13T12:12:46.505579Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03faab4d09ea4ae3bdc44ae2e561e60c"}},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"array([[  0.        ,   0.        ,   0.        ,   0.        ,\n          0.        ,   0.        ],\n       [  2.75200369,   3.94947757,   2.75200369,   3.94947757,\n          5.20997639,  -5.05052243],\n       [  7.93349125,   9.40367562,   5.2099758 ,   9.40365143,\n         10.9512375 ,   0.40367562],\n       ...,\n       [ 10.9512373 ,  12.58025   ,  10.9512375 ,   9.40367561,\n          1.95123746,   1.9512375 ],\n       [ -4.42194461,   6.53681725,   4.35298331,  -4.35891922,\n        -13.22160565, -12.14572498],\n       [ -0.91      ,  -0.973     ,  11.00822128,  18.        ,\n          4.91822128,   0.95421971]])"},"metadata":{}}]},{"cell_type":"code","source":"model = {\n    \"env_id\": env_id,\n    \"max_steps\": max_steps,\n    \"n_training_episodes\": n_training_episodes,\n    \"n_eval_episodes\": n_eval_episodes,\n    \"eval_seed\": eval_seed,\n    \"learning_rate\": learning_rate,\n    \"gamma\": gamma,\n    \"max_epsilon\": max_epsilon,\n    \"min_epsilon\": min_epsilon,\n    \"decay_rate\": decay_rate,\n    \"qtable\": Qtable_taxi,\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:12:52.558679Z","iopub.execute_input":"2024-08-13T12:12:52.559117Z","iopub.status.idle":"2024-08-13T12:12:52.566011Z","shell.execute_reply.started":"2024-08-13T12:12:52.559083Z","shell.execute_reply":"2024-08-13T12:12:52.564603Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"username = \"Leotrim\"\nrepo_name = \"Taxi-v3\"\npush_to_hub(repo_id=f\"{username}/{repo_name}\", model=model, env=env)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:13:34.124040Z","iopub.execute_input":"2024-08-13T12:13:34.124804Z","iopub.status.idle":"2024-08-13T12:13:38.419964Z","shell.execute_reply.started":"2024-08-13T12:13:34.124763Z","shell.execute_reply":"2024-08-13T12:13:38.418565Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e8b9c3d3ae44509f64cb04fcaff294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c6eb990748e406590bb9e1f36bdc443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe24a7a6aa846098739f88a4e2bcb37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28ace3707a9f4f219c2b28788e58e9bf"}},"metadata":{}},{"name":"stdout","text":"False\n","output_type":"stream"},{"name":"stderr","text":"[swscaler @ 0x64903c0] Warning: data is not aligned! This can lead to a speed loss\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"q-learning.pkl:   0%|          | 0.00/24.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c694200cc3a4675b0e8feb33680cf9d"}},"metadata":{}},{"name":"stdout","text":"Your model is pushed to the Hub. You can view your model here:  https://huggingface.co/Leotrim/Taxi-v3\n","output_type":"stream"}]}]}