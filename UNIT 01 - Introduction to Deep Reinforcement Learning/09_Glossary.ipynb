{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary\n",
    "\n",
    "This is a community-created glossary. Contributions are welcome!\n",
    "\n",
    "### Agent\n",
    "\n",
    "Bir ajan, çevreden gelen ödüller ve cezalarla **deneme yanılma yoluyla karar vermeyi öğrenir**.\n",
    "\n",
    "### Environment\n",
    "\n",
    "Ortam, bir ajanın **etkileşime girerek öğrenebileceği simüle edilmiş bir dünyadır**.\n",
    "\n",
    "### Markov Property\n",
    "\n",
    "Ajan tarafından gerçekleştirilen eylemin **sadece mevcut duruma bağlı ve geçmiş durumlardan ve eylemlerden bağımsız** olduğu anlamına gelir.\n",
    "\n",
    "### Observations/State\n",
    "\n",
    "- **State**:  Dünyanın durumunun eksiksiz açıklaması.\n",
    "- **Observation**: Çevrenin/dünyanın durumunun kısmi tanımı.\n",
    "\n",
    "### Actions\n",
    "\n",
    "- **Discrete Actions**: Sol, sağ, yukarı ve aşağı gibi sonlu sayıda eylem.\n",
    "- **Continuous Actions**: Sonsuz eylem olasılığı; örneğin sürücüsüz otomobiller söz konusu olduğunda, sürüş senaryosunda sonsuz eylem gerçekleşme olasılığı vardır.\n",
    "\n",
    "### Rewards and Discounting\n",
    "\n",
    "- **Rewards**: RL'de temel faktör. Temsilciye yapılan eylemin iyi/kötü olup olmadığını söyler.\n",
    "- RL algoritmaları **kümülatif ödülü** maksimize etmeye odaklanır.\n",
    "- **Reward Hypothesis**: RL problemleri (kümülatif) getirinin maksimizasyonu olarak formüle edilebilir.\n",
    "- **Discounting** gerçekleştirilir çünkü başlangıçta elde edilen ödüller, uzun vadeli ödüllerden daha öngörülebilir oldukları için gerçekleşme olasılıkları daha yüksektir.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- **Episodic**: Bir başlangıç noktası ve bir bitiş noktası vardır.\n",
    "- **Continuous**: Bir başlangıç noktası vardır ancak bitiş noktası yoktur.\n",
    "\n",
    "### Exploration v/s Exploitation Trade-Off\n",
    "\n",
    "- **Exploration**: Tamamen rastgele eylemler deneyerek ve çevreden geri bildirim / geri dönüşler / ödüller alarak çevreyi keşfetmekle ilgilidir.\n",
    "- **Exploitation**: Bu, maksimum kazanç elde etmek için çevre hakkında bildiklerimizi kullanmakla ilgilidir.\n",
    "- **Exploration-Exploitation Trade-Off**: Çevreyi ne kadar **keşfetmek** ve çevre hakkında bildiklerimizi ne kadar **istismar etmek** istediğimizi dengeler.\n",
    "\n",
    "### Policy\n",
    "\n",
    "- **Policy**: Buna ajanın beyni denir. Durum göz önüne alındığında bize hangi eylemi gerçekleştirmemiz gerektiğini söyler.\n",
    "- **Optimal Policy**: Bir ajan buna göre hareket ettiğinde **beklenen getiriyi** **maksimize eden** politika. Bu politika **eğitim** yoluyla öğrenilir.\n",
    "\n",
    "### Policy-based Methods:\n",
    "\n",
    "- RL problemlerini çözmeye yönelik bir yaklaşım \n",
    "- Bu yöntemde Politika doğrudan öğrenilir.\n",
    "- Her durumu o duruma karşılık gelen en iyi eylemle eşleştirir. Ya da o durumdaki olası eylemler kümesi üzerinde bir olasılık dağılımı.\n",
    "\n",
    "### Value-based Methods:\n",
    "\n",
    "- RL problemlerini çözmek için başka bir yaklaşım \n",
    "- Burada, bir politikayı eğitmek yerine, her durumu o durumda olmanın beklenen değeriyle eşleştiren bir **değer fonksiyonu** eğitiyoruz.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
