{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The Reinforcement Learning Framework\n",
    "\n",
    "## The RL Process\n",
    "\n",
    "![image5](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg)\n",
    "\n",
    "RL sürecini anlamak için, bir platform oyunu oynamayı öğrenen bir ajan hayal edelim:\n",
    "\n",
    "![image6](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg)\n",
    "\n",
    "- Ajanımız **Environment**'dan $S_0$ durumunu alır - oyunumuzun (Environment) ilk frame'i alırız.\n",
    "- Bu $S_0$ durumuna bağlı olarak Ajan $A_0$ eylemini gerçekleştirir - Ajanımız sağa doğru hareket edecektir.\n",
    "- Environment yeni bir $S_1$ durumuna geçer - yeni frame.\n",
    "- Environment Ajana $R_1$ kadar ödül verir - ölmedik (Pozitif Ödül +1).\n",
    "\n",
    "Bu RL döngüsü state, action, reward ve sonraki state dizisini çıkarır.\n",
    "\n",
    "![image6](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg)\n",
    "\n",
    "Ajanın amacı, beklenen getiri olarak adlandırılan kümülatif ödülünü maksimize etmektir.\n",
    "\n",
    "### The reward hypothesis: the central idea of Reinforcement Learning\n",
    "\n",
    "⇒ Ajanın amacı neden beklenen getiriyi maksimize etmektir?\n",
    "\n",
    "Çünkü RL ödül hipotezine dayanır, yani tüm hedefler beklenen getirinin (beklenen kümülatif ödül) maksimizasyonu olarak tanımlanabilir.\n",
    "\n",
    "Bu nedenle Reinforcement Learning'de, en iyi davranışa sahip olmak için, beklenen kümülatif ödülü en üst düzeye çıkaran eylemlerde bulunmayı öğrenmeyi hedefleriz.\n",
    "\n",
    "### Markov Property\n",
    "\n",
    "Makalelerde, RL sürecinin bir **Markov Decision Process (MDP)** olarak adlandırıldığını göreceksiniz.\n",
    "\n",
    "İlerleyen ünitelerde Markov Özelliği hakkında tekrar konuşacağız. Ancak bugün bununla ilgili bir şeyi hatırlamanız gerekiyorsa, o da şudur: Markov Özelliği, **ajanımızın hangi eylemi gerçekleştireceğine karar vermek için yalnızca mevcut duruma ihtiyaç duyduğunu** ve **daha önce gerçekleştirdiği tüm durumların ve eylemlerin geçmişine ihtiyaç duymadığını ima eder**.\n",
    "\n",
    "### Observations/States Space\n",
    "\n",
    "Observations/States (Gözlemler/Durumlar) ajanımızın çevreden aldığı bilgilerdir. Bir video oyunu söz konusu olduğunda, bu bir kare (ekran görüntüsü) olabilir. Ticaret ajanı söz konusu olduğunda, belirli bir hisse senedinin değeri vb. olabilir.\n",
    "\n",
    "Ancak observation ve state arasında yapılması gereken bir ayrım vardır:\n",
    "\n",
    "`State s`: dünyanın durumunun tam bir açıklamasıdır ( gizlenmiş bilgi yoktur). Tamamen gözlemlenen bir ortamda.\n",
    "\n",
    "![image7](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg)\n",
    "\n",
    "Bir satranç oyununda, tüm tahta bilgilerine erişimimiz vardır, bu nedenle çevreden bir durum alırız. Başka bir deyişle, çevre tamamen gözlemlenir.\n",
    "\n",
    "`Observation o`: durumun kısmi bir açıklamasıdır. Kısmen gözlemlenen bir ortamda.\n",
    "\n",
    "![image8](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg)\n",
    "\n",
    "Super Mario Bros'ta, seviyenin sadece oyuncuya yakın kısmını görürüz, bu yüzden bir gözlem alırız. Super Mario Bros'ta kısmen gözlemlenen bir ortamdayız. Seviyenin sadece bir kısmını gördüğümüz için bir gözlem alırız.\n",
    "\n",
    "Bu derste \"state\" terimini hem state hem de observation'ı ifade etmek için kullanıyoruz, ancak uygulamalarda bu ayrımı yapacağız.\n",
    "\n",
    "Özetlemek gerekirse:\n",
    "\n",
    "![image9](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg)\n",
    "\n",
    "### Action Space\n",
    "\n",
    "Eylem uzayı, bir ortamdaki tüm olası eylemlerin kümesidir.\n",
    "\n",
    "Eylemler ayrık veya sürekli bir uzaydan gelebilir:\n",
    "\n",
    "`Discrete space (Ayrık uzay)`: olası eylemlerin sayısı **sonludur**.\n",
    "\n",
    "![image10](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg)\n",
    "\n",
    "Super Mario Bros'ta, sadece 4 yönümüz olduğu için **sonlu** bir eylem kümesine sahibiz.\n",
    "\n",
    "`Continuous space (Sürekli uzay)`: olası eylemlerin sayısı **sonsuzdur**.\n",
    "\n",
    "![image11](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/self_driving_car.jpg)\n",
    "\n",
    "Bir Self Driving aracının sonsuz sayıda olası eylemi vardır, çünkü 20°, 21,1°, 21,2° sola dönebilir, korna çalabilir, 20° sağa dönebilir...\n",
    "\n",
    "Özetlemek gerekirse:\n",
    "\n",
    "![image12](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg)\n",
    "\n",
    "Bu bilginin dikkate alınması çok önemlidir çünkü gelecekte RL algoritması seçilirken önem taşıyacaktır.\n",
    "\n",
    "### Rewards and the discounting\n",
    "\n",
    "Ödül, RL'de temeldir çünkü ajan için tek geri bildirimdir. Bu sayede, ajanımız yapılan eylemin iyi olup olmadığını bilir.\n",
    "\n",
    "Her t zaman adımındaki kümülatif ödül şu şekilde yazılabilir:\n",
    "\n",
    "![image13](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg)\n",
    "\n",
    "Bu da şuna eşdeğer:\n",
    "\n",
    "![image14](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg)\n",
    "\n",
    "Ancak, gerçekte bunları bu şekilde ekleyemeyiz. Daha erken gelen ödüllerin (oyunun başında) gerçekleşme olasılığı daha yüksektir, çünkü bunlar uzun vadeli gelecekteki ödüllerden daha öngörülebilirdir.\n",
    "\n",
    "Diyelim ki ajanınız her zaman adımında bir taş hareket edebilen küçük bir fare ve rakibiniz de kedi (o da hareket edebiliyor). Farenin amacı, kedi tarafından yenmeden önce maksimum miktarda peynir yemektir.\n",
    "\n",
    "![image15](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg)\n",
    "\n",
    "Diyagramda görebileceğimiz gibi, yakınımızdaki peyniri yemek, kediye yakın olan peyniri yemekten daha olasıdır (kediye ne kadar yakınsak, o kadar tehlikelidir).\n",
    "\n",
    "Sonuç olarak, kedinin yakınındaki ödül, daha büyük (daha fazla peynir) olsa bile, onu yiyebileceğimizden gerçekten emin olmadığımız için daha indirimli olacaktır.\n",
    "\n",
    "Ödülleri indirgemek için şu şekilde ilerliyoruz:\n",
    "\n",
    "1. Gama adı verilen bir indirgeme oranı tanımlarız. Bu oran 0 ile 1 arasında olmalıdır. Çoğu zaman 0,95 ile 0,99 arasındadır.\n",
    "\n",
    "   - Gama ne kadar büyükse, indirim o kadar küçük olur. Bu, ajanımızın uzun vadeli ödülü daha fazla önemsediği anlamına gelir.\n",
    "\n",
    "   - Öte yandan, gama ne kadar küçükse, indirim o kadar büyük olur. Bu da temsilcimizin kısa vadeli ödülü (en yakın peynir) daha fazla önemsediği anlamına gelir.\n",
    "\n",
    "2. Ardından, her ödül zaman adımının üssüne gama ile indirgenecektir. Zaman adımı arttıkça, kedi bize yaklaşır, bu nedenle gelecekteki ödülün gerçekleşme olasılığı gittikçe azalır.\n",
    "\n",
    "İndirgenmiş beklenen kümülatif ödülümüz şudur:\n",
    "\n",
    "![image16](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
