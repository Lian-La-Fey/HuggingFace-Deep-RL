{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summary**\n",
    "\n",
    "Bu çok fazla bilgiydi! Özetleyelim:\n",
    "\n",
    "- Reinforcement Learning, eylemlerden öğrenmenin hesaplamalı bir yaklaşımıdır. **Deneme yanılma yoluyla etkileşime girerek** ve geri bildirim olarak ödüller (negatif veya pozitif) alarak çevreden öğrenen bir ajan inşa ediyoruz.\n",
    "\n",
    "- Herhangi bir RL ajanının amacı, beklenen kümülatif ödülünü (beklenen getiri olarak da adlandırılır) maksimize etmektir çünkü RL, **tüm hedeflerin beklenen kümülatif ödülün maksimizasyonu olarak tanımlanabileceği** **ödül hipotezine** dayanır.\n",
    "\n",
    "- RL süreci, **durum**, **eylem**, **ödül** ve **bir sonraki durum** dizisini çıktı olarak veren bir döngüdür.\n",
    "\n",
    "- Beklenen kümülatif ödülü (beklenen getiri) hesaplamak için ödülleri iskonto ederiz: daha erken gelen ödüllerin (oyunun başında) **gerçekleşme olasılığı daha yüksektir, çünkü uzun vadeli gelecekteki ödülden daha öngörülebilirdirler**.\n",
    "\n",
    "Bir RL problemini çözmek için **en uygun politikayı bulmak istersiniz**. Politika, temsilcinizin \"beynidir\" ve b**ize bir durum karşısında hangi eylemi** gerçekleştireceğimizi söyler. Optimal politika, **size beklenen getiriyi maksimize eden eylemleri veren politikadır**.\n",
    "\n",
    "Optimal politikanızı bulmanın iki yolu vardır:\n",
    "\n",
    "1. Politikanızı doğrudan eğiterek: **politika tabanlı yöntemler**.\n",
    "\n",
    "2. Temsilcinin her durumda elde edeceği beklenen getiriyi bize söyleyen bir değer fonksiyonunu eğiterek ve politikamızı tanımlamak için bu fonksiyonu kullanarak: **değer tabanlı yöntemler**.\n",
    "\n",
    "- Son olarak, Derin RL'den bahsediyoruz çünkü yapılacak eylemi (politika tabanlı) tahmin etmek veya bir durumun değerini (değer tabanlı) tahmin etmek için derin sinir ağlarını tanıtıyoruz, dolayısıyla \"derin\" adı."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
